{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f34ab43",
   "metadata": {},
   "source": [
    "# Tutorial: Using the OKLAD (Oklahoma labeled AI dataset)) Dataset with SeisBench\n",
    "\n",
    "**Author:** Hongyu Xiao @ OU\n",
    "\n",
    "**Last Updated:** 20251117\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f64749-345f-4b2c-8e4b-0e99358cda80",
   "metadata": {},
   "source": [
    "# Even before using seisbench or OKLAD\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose**\n",
    "\n",
    "This notebook is the **very first** step in the SeisBench workflow tutorial, focusing on **loading and reproducing my work**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Machiene Learning Key Package Version and Setup**\n",
    "\n",
    "| Component | Detail |\n",
    "| :--- | :--- |\n",
    "| **python version** | 3.12 |\n",
    "| **seisbench version** | 0.7.0 |\n",
    "| **cuda version** | 12.1 |\n",
    "| **jupyter version** | 1.0.0 |\n",
    "| **obspy version** | 1.4.1 |\n",
    "| **torch version** | 2.3.1 |\n",
    "\n",
    "### **(*) Optional but helpful packages for visualization**\n",
    "\n",
    "| Component | Detail |\n",
    "| :--- | :--- |\n",
    "| **seaborn version** | 0.13.2 |\n",
    "| **matplotlib version** | 3.9.0 |\n",
    "| **cartopy version** | 0.24.1 |\n",
    "\n",
    "### **(*) Annotation packages**\n",
    "\n",
    "| Component | Detail |\n",
    "| :--- | :--- |\n",
    "| **pyocto version** | 0.1.9 |\n",
    "| **pyarrow version** | 19.0.0 |\n",
    "| **pyrocko version** | 2025.1.21 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b75c782",
   "metadata": {},
   "source": [
    "If you do not know what is your current version situation, copy the following code run in your notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249ffe49-37c5-4ec5-acc2-fdb774cd91b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.12.3\n",
      "seisbench: 0.7.0\n",
      "cuda: N/A\n",
      "jupyter: 5.7.2\n",
      "obspy: 1.4.1\n",
      "torch: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import seisbench\n",
    "import obspy\n",
    "import torch\n",
    "import jupyter_core\n",
    "\n",
    "print(f\"python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\") # Print the major, minor, and micro version of the running Python interpreter.\n",
    "print(f\"seisbench: {seisbench.__version__}\") # Print the version of the SeisBench library.\n",
    "print(f\"cuda: {torch.version.cuda if torch.cuda.is_available() else 'N/A'}\") # Print the version of the CUDA toolkit being used by PyTorch, or 'N/A' if CUDA is not available.\n",
    "print(f\"jupyter: {jupyter_core.__version__}\") # Print the version of the Jupyter Core library.\n",
    "print(f\"obspy: {obspy.__version__}\") # Print the version of the ObsPy library.\n",
    "print(f\"torch: {torch.__version__}\") # Print the version of the PyTorch library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a995433-1c1b-4367-8167-1d099f700d37",
   "metadata": {},
   "source": [
    "You could copy and paste the following code to install the most essential ones first\n",
    "\n",
    "```python\n",
    "\n",
    "!pip install python==3.12 seisbench==0.7.0 cuda==12.1 obspy==1.4.1 torch==2.3.1\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31946cd-7edc-49de-af63-ae177ae41a8d",
   "metadata": {},
   "source": [
    "Other versions of the packages should be ok if they are backward compatible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c86a8ac",
   "metadata": {},
   "source": [
    "## Loading a Dataset\n",
    "\n",
    "SeisBench provides access to several pre-compiled datasets. These are curated collections of seismic waveforms and associated metadata, ready for use in machine learning applications. You can find a list of available datasets in the [SeisBench documentation](https://seisbench.readthedocs.io/en/stable/pages/benchmark_datasets.html).\n",
    "\n",
    "Here, we first will load the \"DummyDataset\" dataset, which is a sample dataset in seismology. We specify a `sampling_rate` of 100 Hz, which means the waveforms will be resampled to this frequency if they are not already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4319e180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 16:07:03,141 | seisbench | WARNING | Component order not specified, defaulting to 'ZNE'.\n"
     ]
    }
   ],
   "source": [
    "import seisbench\n",
    "import seisbench.data as sbd\n",
    "\n",
    "data = sbd.DummyDataset(sampling_rate=100)\n",
    "train, dev, test = data.train_dev_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c1af1",
   "metadata": {},
   "source": [
    "When running this command for the first time, the dataset is downloaded. All downloaded data is stored in the SeisBench cache. \n",
    "\n",
    "The location of the cache defaults to `~/.seisbench`, but can be set using the environment variable `SEISBENCH_CACHE_ROOT`. \n",
    "\n",
    "Let's inspect the cache. Depending which commands where used before, it contains at least the directory `datasets`. \n",
    "\n",
    "Inside this directory, each locally available dataset has its own folder. If we look into the folder `dummydataset`, we find two relevant files `metadata.csv` and `waveforms.hdf5`, containing the metadata and the waveforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74ab568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/hongyuxiao/.seisbench\n",
      "\u001b[34mdummydataset\u001b[m\u001b[m/         \u001b[34mokla_1mil_120s_ver_3\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%cd ~/.seisbench\n",
    "\n",
    "%ls ~/.seisbench/datasets/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077f547c",
   "metadata": {},
   "source": [
    "Within the .seisbench directory, here would be how it looks like:\n",
    "```\n",
    "├── config.json\n",
    "├── datasets\n",
    "│   ├── dummydataset\n",
    "│   │   ├── metadata.csv\n",
    "│   │   └── waveforms.hdf5\n",
    "│   └── okla_1mil_120s_ver_3\n",
    "│       ├── metadata.csv\n",
    "│       └── waveforms.hdf5\n",
    "└── models\n",
    "    └── v3\n",
    "        ├── eqtransformer\n",
    "        │   ├── instance.json.v2\n",
    "        │   ├── instance.pt.v2\n",
    "        │   ├── original.json.v3\n",
    "        │   └── original.pt.v3\n",
    "        ├── gpd\n",
    "        │   ├── ethz.json.v1\n",
    "        │   ├── ethz.pt.v1\n",
    "        │   ├── original.json.v1\n",
    "        │   ├── original.pt.v1\n",
    "        └── phasenet\n",
    "            ├── instance.json.v2\n",
    "            ├── instance.pt.v2\n",
    "            ├── original.json.v1\n",
    "            ├── original.pt.v1\n",
    "            ├── scedc.json.v2\n",
    "            ├── scedc.pt.v2\n",
    "            ├── stead.json.v2\n",
    "            └── stead.pt.v2\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de4a2c",
   "metadata": {},
   "source": [
    "## What does a dataset contain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51afef0",
   "metadata": {},
   "source": [
    "Each dataset consists of waveforms and the associated metadata.\n",
    "\n",
    "Let's first inspect the metadata. It is represented by a **pandas** DataFrame and lists for each trace different attributes, describing properties of the source, the trace, the station and possibly the path. When loading a dataset, only the metadata is loaded into memory. \n",
    "\n",
    "The waveforms are loaded on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "257ed22b-9fa5-43b0-b2e4-adc2e733623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 16:17:52,164 | seisbench | WARNING | Component order not specified, defaulting to 'ZNE'.\n"
     ]
    }
   ],
   "source": [
    "import seisbench\n",
    "import seisbench.data as sbd\n",
    "\n",
    "data = sbd.DummyDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f022574c-4f57-41be-96eb-2cc53519fa6c",
   "metadata": {},
   "source": [
    "## This dataset contains 100 traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f73cb9dd-2815-4391-b2af-d190a4c3fd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DummyDataset - 100 traces\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c5e64bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>trace_start_time</th>\n",
       "      <th>source_latitude_deg</th>\n",
       "      <th>source_longitude_deg</th>\n",
       "      <th>source_depth_km</th>\n",
       "      <th>source_event_category</th>\n",
       "      <th>source_magnitude</th>\n",
       "      <th>source_magnitude_uncertainty</th>\n",
       "      <th>source_magnitude2</th>\n",
       "      <th>source_magnitude_uncertainty2</th>\n",
       "      <th>...</th>\n",
       "      <th>station_latitude_deg</th>\n",
       "      <th>station_longitude_deg</th>\n",
       "      <th>station_elevation_m</th>\n",
       "      <th>source_magnitude_type</th>\n",
       "      <th>source_magnitude_type2</th>\n",
       "      <th>split</th>\n",
       "      <th>trace_name_original</th>\n",
       "      <th>trace_chunk</th>\n",
       "      <th>trace_sampling_rate_hz</th>\n",
       "      <th>trace_component_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2007/01/01 01:42:45.08</td>\n",
       "      <td>-20.43802</td>\n",
       "      <td>-69.27681</td>\n",
       "      <td>83.18</td>\n",
       "      <td>ID</td>\n",
       "      <td>1.353</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.426</td>\n",
       "      <td>0.011</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.04323</td>\n",
       "      <td>-69.4874</td>\n",
       "      <td>900.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>ML</td>\n",
       "      <td>train</td>\n",
       "      <td>2007_01_01 01_42_45_08</td>\n",
       "      <td></td>\n",
       "      <td>20</td>\n",
       "      <td>ZNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2007/01/01 02:41:13.75</td>\n",
       "      <td>-21.64059</td>\n",
       "      <td>-68.41443</td>\n",
       "      <td>118.38</td>\n",
       "      <td>ID</td>\n",
       "      <td>1.981</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.027</td>\n",
       "      <td>0.023</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.04323</td>\n",
       "      <td>-69.4874</td>\n",
       "      <td>900.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>ML</td>\n",
       "      <td>train</td>\n",
       "      <td>2007_01_01 02_41_13_75</td>\n",
       "      <td></td>\n",
       "      <td>20</td>\n",
       "      <td>ZNE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2007/01/01 03:50:29.27</td>\n",
       "      <td>-21.84637</td>\n",
       "      <td>-68.53904</td>\n",
       "      <td>111.82</td>\n",
       "      <td>ID</td>\n",
       "      <td>2.719</td>\n",
       "      <td>0.024</td>\n",
       "      <td>2.811</td>\n",
       "      <td>0.026</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.04323</td>\n",
       "      <td>-69.4874</td>\n",
       "      <td>900.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>ML</td>\n",
       "      <td>train</td>\n",
       "      <td>2007_01_01 03_50_29_27</td>\n",
       "      <td></td>\n",
       "      <td>20</td>\n",
       "      <td>ZNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        trace_start_time  source_latitude_deg  source_longitude_deg  \\\n",
       "0      0  2007/01/01 01:42:45.08            -20.43802             -69.27681   \n",
       "1      1  2007/01/01 02:41:13.75            -21.64059             -68.41443   \n",
       "2      2  2007/01/01 03:50:29.27            -21.84637             -68.53904   \n",
       "\n",
       "   source_depth_km source_event_category  source_magnitude  \\\n",
       "0            83.18                    ID             1.353   \n",
       "1           118.38                    ID             1.981   \n",
       "2           111.82                    ID             2.719   \n",
       "\n",
       "   source_magnitude_uncertainty  source_magnitude2  \\\n",
       "0                         0.014              1.426   \n",
       "1                         0.020              2.027   \n",
       "2                         0.024              2.811   \n",
       "\n",
       "   source_magnitude_uncertainty2  ... station_latitude_deg  \\\n",
       "0                          0.011  ...            -21.04323   \n",
       "1                          0.023  ...            -21.04323   \n",
       "2                          0.026  ...            -21.04323   \n",
       "\n",
       "  station_longitude_deg station_elevation_m source_magnitude_type  \\\n",
       "0              -69.4874               900.0                    MA   \n",
       "1              -69.4874               900.0                    MA   \n",
       "2              -69.4874               900.0                    MA   \n",
       "\n",
       "   source_magnitude_type2  split     trace_name_original trace_chunk  \\\n",
       "0                      ML  train  2007_01_01 01_42_45_08               \n",
       "1                      ML  train  2007_01_01 02_41_13_75               \n",
       "2                      ML  train  2007_01_01 03_50_29_27               \n",
       "\n",
       "  trace_sampling_rate_hz trace_component_order  \n",
       "0                     20                   ZNE  \n",
       "1                     20                   ZNE  \n",
       "2                     20                   ZNE  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.metadata[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392815d5-0537-428e-8a9a-8a73f5afc7df",
   "metadata": {},
   "source": [
    "The first time when you are loading pre-compiled dataset or your own dataset, it might take a while. \n",
    "\n",
    "seisbench will recompile your csv and hdf5 file into its onw data format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc8a5f-7f09-479b-a85a-d267a1aecc88",
   "metadata": {},
   "source": [
    "# Load your own dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5585621c-e425-4ce1-849e-b2883c89f9a4",
   "metadata": {},
   "source": [
    "If you want to load your own data, it might takes a bit more tweaking. But it is very simple. \n",
    "\n",
    "Here is an example.\n",
    "\n",
    "First you want to check your seisbench location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c79072f-f3bc-4ef6-a788-ad86820836fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: seisbench\n",
      "Version: 0.7.0\n",
      "Summary: The seismological machine learning benchmark collection\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Jack Woolam <jack.woollam@kit.edu>, Jannes Münchmeyer <munchmej@gfz-potsdam.de>\n",
      "License: GPLv3\n",
      "Location: /opt/anaconda3/envs/ML/lib/python3.12/site-packages\n",
      "Requires: bottleneck, h5py, nest-asyncio, numpy, obspy, pandas, scipy, torch, tqdm\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show seisbench"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa2b28a-88cb-406d-b71a-fe7aaf5cd7ea",
   "metadata": {},
   "source": [
    "\n",
    "For this example, it shows that seisbench is installed at \n",
    "\n",
    "<mark> /opt/anaconda3/envs/ML/lib/python3.12/site-packages/seisbench/ </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07746c-4f57-4802-959e-2c638c7c849f",
   "metadata": {},
   "source": [
    "get into that directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b53b63e7-7cf9-4a53-a140-330ea088e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py  \u001b[34m__pycache__\u001b[m\u001b[m/ \u001b[34mdata\u001b[m\u001b[m/        \u001b[34mgenerate\u001b[m\u001b[m/    \u001b[34mmodels\u001b[m\u001b[m/      \u001b[34mutil\u001b[m\u001b[m/\n"
     ]
    }
   ],
   "source": [
    "%ls /opt/anaconda3/envs/ML/lib/python3.12/site-packages/seisbench/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a295dfe-6d83-456f-a40f-f5f1b704d394",
   "metadata": {},
   "source": [
    "This is how the directory looks like (only showing direcotry here)\n",
    "\n",
    "```\n",
    "/opt/anaconda3/envs/ML/lib/python3.12/site-packages/seisbench/\n",
    "├── __pycache__\n",
    "├── data\n",
    "│   └── __pycache__\n",
    "├── generate\n",
    "│   └── __pycache__\n",
    "├── models\n",
    "│   └── __pycache__\n",
    "└── util\n",
    "    └── __pycache__\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bf7e64",
   "metadata": {},
   "source": [
    "## There will be a ```__init__.py``` will be under root directory <mark>/opt/anaconda3/envs/ML/lib/python3.12/site-packages/seisbench/</mark>, that is the place where you could change ```cache_root``` variable to decicde where to put your seisbench data and models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dad9ba",
   "metadata": {},
   "source": [
    "## Now, lets move into `data` folder <mark>/opt/anaconda3/envs/ML/lib/python3.12/site-packages/seisbench/data</mark> , will be **another** ```__init__.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08284148-d208-45b5-b169-e8f023050a88",
   "metadata": {},
   "source": [
    "under the folder <mark>data</mark>, you will find a **py** file called ```__init__.py```\n",
    "\n",
    "```\n",
    "/opt/anaconda3/envs/ML/lib/python3.12/site-packages/seisbench/data/__init__.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e5d82c-9fd6-49ff-9765-27ec14349768",
   "metadata": {},
   "source": [
    "And this will be how it looks like \n",
    "\n",
    "``` \n",
    "from .base import (\n",
    "    BenchmarkDataset,\n",
    "    Bucketer,\n",
    "    GeometricBucketer,\n",
    "    MultiWaveformDataset,\n",
    "    WaveformDataset,\n",
    "    WaveformDataWriter,\n",
    ")\n",
    "from .dummy import ChunkedDummyDataset, DummyDataset\n",
    "from .ethz import ETHZ\n",
    "from .geofon import GEOFON\n",
    "from .instance import InstanceCounts, InstanceCountsCombined, InstanceGM, InstanceNoise\n",
    "from .iquique import Iquique\n",
    "from .isc_ehb import ISC_EHB_DepthPhases\n",
    "from .lendb import LenDB\n",
    "from .lfe_stacks import (\n",
    "    LFEStacksCascadiaBostock2015,\n",
    "    LFEStacksMexicoFrank2014,\n",
    "    LFEStacksSanAndreasShelly2017,\n",
    ")\n",
    "from .neic import MLAAPDE, NEIC\n",
    "from .obs import OBS\n",
    "from .obst2024 import OBST2024\n",
    "from .pnw import PNW, PNWAccelerometers, PNWExotic, PNWNoise\n",
    "from .scedc import SCEDC, Meier2019JGR, Ross2018GPD, Ross2018JGRFM, Ross2018JGRPick\n",
    "from .stead import STEAD\n",
    "from .txed import TXED\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cbc7d7-34e1-426e-8cbc-92530bc7e7cd",
   "metadata": {},
   "source": [
    "Then you want to add your own customized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07795415-7eee-4941-b827-9830de43bf08",
   "metadata": {},
   "source": [
    "For example, here is what I added after the last line.\n",
    "\n",
    "```\n",
    "# By Hongyu Xiao, This is intended for New Million Dataset with Longer Trace length of 120s\n",
    "from .okla_1Mil_120_ver3 import OKLA_1Mil_120s_Ver_3```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b02b6af-b045-4c3b-83b1-da0d274696c5",
   "metadata": {},
   "source": [
    "In this case, this is how my ```/opt/anaconda3/envs/ML/lib/python3.12/site-packages/seisbench/data/__init__.py``` looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60929384-d573-47ac-9d9e-3141205f5354",
   "metadata": {},
   "source": [
    "```\n",
    "from .base import (\n",
    "    BenchmarkDataset,\n",
    "    Bucketer,\n",
    "    GeometricBucketer,\n",
    "    MultiWaveformDataset,\n",
    "    WaveformDataset,\n",
    "    WaveformDataWriter,\n",
    ")\n",
    "from .dummy import ChunkedDummyDataset, DummyDataset\n",
    "from .ethz import ETHZ\n",
    "from .geofon import GEOFON\n",
    "from .instance import InstanceCounts, InstanceCountsCombined, InstanceGM, InstanceNoise\n",
    "from .iquique import Iquique\n",
    "from .isc_ehb import ISC_EHB_DepthPhases\n",
    "from .lendb import LenDB\n",
    "from .lfe_stacks import (\n",
    "    LFEStacksCascadiaBostock2015,\n",
    "    LFEStacksMexicoFrank2014,\n",
    "    LFEStacksSanAndreasShelly2017,\n",
    ")\n",
    "from .neic import MLAAPDE, NEIC\n",
    "from .obs import OBS\n",
    "from .obst2024 import OBST2024\n",
    "from .pnw import PNW, PNWAccelerometers, PNWExotic, PNWNoise\n",
    "from .scedc import SCEDC, Meier2019JGR, Ross2018GPD, Ross2018JGRFM, Ross2018JGRPick\n",
    "from .stead import STEAD\n",
    "from .txed import TXED\n",
    "\n",
    "# By Hongyu Xiao, This is intended for New Million Dataset with Longer Trace length of 120s\n",
    "from .okla_1Mil_120_ver3 import OKLA_1Mil_120s_Ver_3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c98e38f-c99b-420c-be9b-ae392630a121",
   "metadata": {},
   "source": [
    "in my case, \n",
    "\n",
    "under the directory: ```/opt/anaconda3/envs/ML/lib/python3.12/site-packages/seisbench/data/``` \n",
    "\n",
    "compose a **okla_1Mil_120_ver3.py** file that could be imported when using this package."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba429b-fe9d-4d43-9919-cad1a8879afb",
   "metadata": {},
   "source": [
    "The key is that in the line\n",
    "\n",
    "<mark>from **.okla_1Mil_120_ver3** import OKLA_1Mil_120s_Ver_3 \n",
    "\n",
    "the name should be the same as the added py file.\n",
    "\n",
    "<mark>**okla_1Mil_120_ver3.py**<mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c423113-49d2-40ef-ab18-695c7461ab57",
   "metadata": {},
   "source": [
    "And this is how my okla_1Mil_120_ver3.py looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9548ac-dc3a-47bd-ac49-08133318e0a9",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "import seisbench\n",
    "import seisbench.util\n",
    "from .base import BenchmarkDataset, WaveformDataWriter\n",
    "\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class OKLA_1Mil_120s_Ver_3(BenchmarkDataset):\n",
    "    \"\"\"\n",
    "    OKLAHOMA dataset , Hongyu Xiao\n",
    "\n",
    "    Using the train/test split from the EQTransformer Github repository\n",
    "    train/dev split defined in SeisBench\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        citation = (\n",
    "            \"Hongyu Xiao Modified from STanford EArthquake Dataset (STEAD): \"\n",
    "            \"doi:\"\n",
    "        )\n",
    "        license = \"CC BY 4.0\"\n",
    "        super().__init__(citation=citation, license=license, **kwargs)\n",
    "\n",
    "    def _download_dataset(self, writer: WaveformDataWriter, basepath=None, **kwargs):\n",
    "        download_instructions = (\n",
    "            \"Please download STEAD following the instructions at https://github.com/smousavi05/STEAD. \"\n",
    "            \"Provide the locations of the STEAD formatted unpacked files (metadata.csv and waveform.hdf5) in the \"\n",
    "            \"download_kwargs argument 'basepath'.\"\n",
    "            \"This step is only necessary the first time STEAD is loaded.\"\n",
    "        )\n",
    "\n",
    "        metadata_dict = {\n",
    "            \"trace_start_time\": \"trace_start_time\",\n",
    "            \"trace_category\": \"trace_category\",\n",
    "            \"trace_name\": \"trace_name\",\n",
    "            \"p_arrival_sample\": \"trace_p_arrival_sample\",\n",
    "            \"p_status\": \"trace_p_status\",\n",
    "            \"p_weight\": \"trace_p_weight\",\n",
    "            \"p_travel_sec\": \"path_p_travel_sec\",\n",
    "            \"s_arrival_sample\": \"trace_s_arrival_sample\",\n",
    "            \"s_status\": \"trace_s_status\",\n",
    "            \"s_weight\": \"trace_s_weight\",\n",
    "            \"s_travel_sec\": \"path_s_travel_sec\",\n",
    "            \"back_azimuth_deg\": \"path_back_azimuth_deg\",\n",
    "            \"snr_db\": \"trace_snr_db\",\n",
    "            \"coda_end_sample\": \"trace_coda_end_sample\",\n",
    "            \"network_code\": \"station_network_code\",\n",
    "            \"receiver_code\": \"station_code\",\n",
    "            \"receiver_type\": \"trace_channel\",\n",
    "            \"receiver_latitude\": \"station_latitude_deg\",\n",
    "            \"receiver_longitude\": \"station_longitude_deg\",\n",
    "            \"receiver_elevation_m\": \"station_elevation_m\",\n",
    "            \"source_id\": \"source_id\",\n",
    "            \"source_origin_time\": \"source_origin_time\",\n",
    "            \"source_origin_uncertainty_sec\": \"source_origin_uncertainty_sec\",\n",
    "            \"source_latitude\": \"source_latitude_deg\",\n",
    "            \"source_longitude\": \"source_longitude_deg\",\n",
    "            \"source_error_sec\": \"source_error_sec\",\n",
    "            \"source_gap_deg\": \"source_gap_deg\",\n",
    "            \"source_horizontal_uncertainty_km\": \"source_horizontal_uncertainty_km\",\n",
    "            \"source_depth_km\": \"source_depth_km\",\n",
    "            \"source_depth_uncertainty_km\": \"source_depth_uncertainty_km\",\n",
    "            \"source_magnitude\": \"source_magnitude\",\n",
    "            \"source_magnitude_type\": \"source_magnitude_type\",\n",
    "            \"source_magnitude_author\": \"source_magnitude_author\",\n",
    "        }\n",
    "\n",
    "        path = self.path\n",
    "        #basepath = '/Users/hongyuxiao/Hongyu_File/Data'\n",
    "        basepath = '/ourdisk/hpc/ogs/hongyux/dont_archive/Merge_Data_OklaCLEAN_1Mil_Raw_Ver_3_with_120s'\n",
    "        #basepath = '/ourdisk/hpc/ogs/hongyux/dont_archive/Merge_Data_OklaCLEAN_1Mil_Raw_Ver2'\n",
    "        if basepath is None:\n",
    "            raise ValueError(\n",
    "                \"No cached version of Okla_1Mil_120 found. \" + download_instructions\n",
    "            )\n",
    "\n",
    "        basepath = Path(basepath)\n",
    "\n",
    "        if not (basepath / \"merge.csv\").is_file():\n",
    "            raise ValueError(\n",
    "                \"Basepath does not contain file merged.csv. \" + download_instructions\n",
    "            )\n",
    "        if not (basepath / \"merge.hdf5\").is_file():\n",
    "            raise ValueError(\n",
    "                \"Basepath does not contain file merge.hdf5. \" + download_instructions\n",
    "            )\n",
    "\n",
    "        self.path.mkdir(parents=True, exist_ok=True)\n",
    "        seisbench.logger.warning(\n",
    "            \"Converting Okla_1Mil_120 files to SeisBench format. This might take a while.\"\n",
    "        )\n",
    "\n",
    "        #split_url = \"https://github.com/smousavi05/EQTransformer/raw/master/ModelsAndSampleData/test.npy\"\n",
    "        #seisbench.util.download_http(\n",
    "        #    split_url, path / \"test.npy\", desc=f\"Downloading test splits\"\n",
    "        #)\n",
    "\n",
    "        # Copy metadata and rename columns to SeisBench format\n",
    "        metadata = pd.read_csv(basepath / \"merge.csv\")\n",
    "        metadata.rename(columns=metadata_dict, inplace=True)\n",
    "        metadata['split'] = np.random.choice(['train', 'dev', 'test'], size=metadata.shape[0], p=[0.7, 0.15, 0.15])\n",
    "\n",
    "\t#splits = []\n",
    "        #for o in metadata['source_origin_time']: \n",
    "        #    if str(o) < \"2021-01-01\": \n",
    "        #        split = \"train\" \n",
    "        #    elif str(o) < \"2021-06-01\": \n",
    "        #        split = \"dev\" \n",
    "        #    else: \n",
    "        #        split = \"test\" \n",
    "        #    splits.append(split) \n",
    "        #metadata[\"split\"] = splits\n",
    "        # Set split\n",
    "        #test_split = set(np.load(path / \"test.npy\"))\n",
    "        #test_mask = metadata[\"trace_name\"].isin(test_split)\n",
    "        #train_dev = metadata[\"trace_name\"][~test_mask].values\n",
    "        #dev_split = train_dev[\n",
    "        #    ::18\n",
    "        #]  # Use 5% of total traces as suggested in EQTransformer Github repository\n",
    "        #dev_mask = metadata[\"trace_name\"].isin(dev_split)\n",
    "        #metadata[\"split\"] = \"train\"\n",
    "        #metadata.loc[dev_mask, \"split\"] = \"dev\"\n",
    "        #metadata.loc[test_mask, \"split\"] = \"test\"\n",
    "\n",
    "        # Writer data format\n",
    "        writer.data_format = {\n",
    "            \"dimension_order\": \"CW\",\n",
    "            \"component_order\": \"ZNE\",\n",
    "            \"sampling_rate\": 100,\n",
    "            \"measurement\": \"velocity\",\n",
    "            \"unit\": \"counts\",\n",
    "            \"instrument_response\": \"not restituted\",\n",
    "        }\n",
    "\n",
    "        writer.set_total(len(metadata))\n",
    "\n",
    "        with h5py.File(basepath / \"merge.hdf5\") as f:\n",
    "            gdata = f[\"data\"]\n",
    "            for _, row in metadata.iterrows():\n",
    "                row = row.to_dict()\n",
    "                #print(_,row)\n",
    "                #print(waveforms)\n",
    "                waveforms = gdata[row[\"trace_name\"]][()]\n",
    "                #print(waveforms)\n",
    "                #print(type(waveforms))\n",
    "                #print(len(waveforms))\n",
    "                if waveforms.shape[1] == 3:\n",
    "                    waveforms = waveforms.T  # From WC to CW\n",
    "                    waveforms = waveforms[[2, 1, 0]]  # From ENZ to ZNE\n",
    "\n",
    "                    writer.add_trace(row, waveforms)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcf29e5-6169-4efa-826f-04f3711660ee",
   "metadata": {},
   "source": [
    "## Documentation of okla_1Mil_120_ver3.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63196ede-65da-4681-908a-a5bf7fa57631",
   "metadata": {},
   "source": [
    "<mark>class OKLA_1Mil_120s_Ver_3(BenchmarkDataset):</mark>\n",
    "\n",
    "- Please make sure OKLA_1Mil_120s_Ver_3 is same name as what you added in ```__init__.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99001c4-787a-4b4a-a2b9-d4e1d5f1551f",
   "metadata": {},
   "source": [
    "metadata_dict should be adjusted accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ac8936-229a-456c-9ab7-896fb67e7950",
   "metadata": {},
   "source": [
    "Make sure this was linked properly to the location of you complied csv and hdf5 file directory\n",
    "\n",
    " - ```basepath = '/ourdisk/hpc/ogs/hongyux/dont_archive/Merge_Data_OklaCLEAN_1Mil_Raw_Ver_3_with_120s'```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc07d0b-9be1-444e-85a5-de8eaf423f88",
   "metadata": {},
   "source": [
    "```metadata['split'] = np.random.choice(['train', 'dev', 'test'], size=metadata.shape[0], p=[0.7, 0.15, 0.15])```\n",
    "\n",
    "This will randomly split the dataset into <mark>70% training, 15% dev and 15% test </mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b816ac-8378-41fe-aa2b-eb2b125208af",
   "metadata": {},
   "source": [
    "```# Writer data format\n",
    "    writer.data_format = {\n",
    "        \"dimension_order\": \"CW\",\n",
    "        \"component_order\": \"ZNE\",\n",
    "        \"sampling_rate\": 100,\n",
    "        \"measurement\": \"velocity\",\n",
    "        \"unit\": \"counts\",\n",
    "        \"instrument_response\": \"not restituted\",\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2361fbb-b21f-4403-8d57-8bac83ea7821",
   "metadata": {},
   "source": [
    "Please adjust this component accordingly as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda2b5a4-30ad-4280-84f3-eba071d32a07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
