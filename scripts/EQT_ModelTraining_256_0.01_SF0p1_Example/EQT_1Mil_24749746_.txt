/home/hongyux/.conda/envs/TL/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.
  warnings.warn("The verbose parameter is deprecated. Please use get_last_lr() "
Using device: cuda:0
Loading data...
Creating random sample of 10.0% of the data...
Sampled dataset size: 113853
Train: OKLA_1Mil_120s_Ver_3 - 79525 traces
Dev: OKLA_1Mil_120s_Ver_3 - 17058 traces
Test: OKLA_1Mil_120s_Ver_3 - 17270 traces
Example: {'X': array([[ 4.9953605e-04,  2.0856745e-04, -1.2997282e-03, ...,
        -2.3670229e-03, -2.6579914e-03,  8.5450163e-05],
       [ 1.2853845e-03, -1.6954173e-04, -2.3812300e-04, ...,
        -1.3383226e-03, -1.8722847e-03,  6.0648151e-04],
       [ 7.4030454e-03,  4.6269228e-03, -1.3126903e-03, ...,
        -7.2806492e-03, -6.5919957e-03,  1.1122910e-03]], dtype=float32), 'y': array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [1., 1., 1., ..., 1., 1., 1.]]), 'detections': array([[0., 0., 0., ..., 0., 0., 0.]])}
Example: [[ 4.9953605e-04  2.0856745e-04 -1.2997282e-03 ... -2.3670229e-03
  -2.6579914e-03  8.5450163e-05]
 [ 1.2853845e-03 -1.6954173e-04 -2.3812300e-04 ... -1.3383226e-03
  -1.8722847e-03  6.0648151e-04]
 [ 7.4030454e-03  4.6269228e-03 -1.3126903e-03 ... -7.2806492e-03
  -6.5919957e-03  1.1122910e-03]]
Example: (3, 6000)
Example: [[0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [1. 1. 1. ... 1. 1. 1.]]
Example: (3, 6000)
Example: [[0. 0. 0. ... 0. 0. 0.]]
Epoch 1/50
loss: 0.163252 [    0/79525]
loss: 0.056491 [ 2560/79525]
loss: 0.051390 [ 5120/79525]
loss: 0.045166 [ 7680/79525]
loss: 0.042938 [10240/79525]
loss: 0.039620 [12800/79525]
loss: 0.035808 [15360/79525]
loss: 0.033356 [17920/79525]
loss: 0.032434 [20480/79525]
loss: 0.031323 [23040/79525]
loss: 0.030557 [25600/79525]
loss: 0.030937 [28160/79525]
loss: 0.031674 [30720/79525]
loss: 0.029301 [33280/79525]
loss: 0.029411 [35840/79525]
loss: 0.030029 [38400/79525]
loss: 0.028147 [40960/79525]
loss: 0.028531 [43520/79525]
loss: 0.027845 [46080/79525]
loss: 0.027926 [48640/79525]
loss: 0.027109 [51200/79525]
loss: 0.025852 [53760/79525]
loss: 0.026948 [56320/79525]
loss: 0.027532 [58880/79525]
loss: 0.025962 [61440/79525]
loss: 0.026275 [64000/79525]
loss: 0.026337 [66560/79525]
loss: 0.022595 [69120/79525]
loss: 0.024134 [71680/79525]
loss: 0.024223 [74240/79525]
loss: 0.025555 [76800/79525]
loss: 0.024492 [51150/79525]
Epoch 1 results: Train loss: 0.033232, Val loss: 0.023925
Validation loss decreased (inf --> 0.023925). Saving model...
Saving best model to best_model.pth
Epoch 2/50
loss: 0.024533 [    0/79525]
loss: 0.024246 [ 2560/79525]
loss: 0.023785 [ 5120/79525]
loss: 0.025078 [ 7680/79525]
loss: 0.023855 [10240/79525]
loss: 0.024345 [12800/79525]
loss: 0.024721 [15360/79525]
loss: 0.023645 [17920/79525]
loss: 0.023304 [20480/79525]
loss: 0.022449 [23040/79525]
loss: 0.024031 [25600/79525]
loss: 0.023501 [28160/79525]
loss: 0.024400 [30720/79525]
loss: 0.023360 [33280/79525]
loss: 0.022134 [35840/79525]
loss: 0.023410 [38400/79525]
loss: 0.021820 [40960/79525]
loss: 0.020762 [43520/79525]
loss: 0.021466 [46080/79525]
loss: 0.022994 [48640/79525]
loss: 0.022044 [51200/79525]
loss: 0.022203 [53760/79525]
loss: 0.022275 [56320/79525]
loss: 0.023556 [58880/79525]
loss: 0.021576 [61440/79525]
loss: 0.020425 [64000/79525]
loss: 0.021223 [66560/79525]
loss: 0.020836 [69120/79525]
loss: 0.021644 [71680/79525]
loss: 0.021826 [74240/79525]
loss: 0.021860 [76800/79525]
loss: 0.019819 [51150/79525]
Epoch 2 results: Train loss: 0.022894, Val loss: 0.021400
Validation loss decreased (0.023925 --> 0.021400). Saving model...
Saving best model to best_model.pth
Epoch 3/50
loss: 0.021778 [    0/79525]
loss: 0.021575 [ 2560/79525]
loss: 0.022402 [ 5120/79525]
loss: 0.020756 [ 7680/79525]
loss: 0.021376 [10240/79525]
loss: 0.021316 [12800/79525]
loss: 0.019956 [15360/79525]
loss: 0.020194 [17920/79525]
loss: 0.022708 [20480/79525]
loss: 0.020993 [23040/79525]
loss: 0.021196 [25600/79525]
loss: 0.022686 [28160/79525]
loss: 0.021392 [30720/79525]
loss: 0.021087 [33280/79525]
loss: 0.021825 [35840/79525]
loss: 0.020776 [38400/79525]
loss: 0.020701 [40960/79525]
loss: 0.022085 [43520/79525]
loss: 0.020813 [46080/79525]
loss: 0.020824 [48640/79525]
loss: 0.019976 [51200/79525]
loss: 0.021183 [53760/79525]
loss: 0.020978 [56320/79525]
loss: 0.021298 [58880/79525]
loss: 0.021608 [61440/79525]
loss: 0.020606 [64000/79525]
loss: 0.019689 [66560/79525]
loss: 0.021210 [69120/79525]
loss: 0.020536 [71680/79525]
loss: 0.020087 [74240/79525]
loss: 0.019701 [76800/79525]
loss: 0.021129 [51150/79525]
Epoch 3 results: Train loss: 0.021038, Val loss: 0.020294
Validation loss decreased (0.021400 --> 0.020294). Saving model...
Saving best model to best_model.pth
Epoch 4/50
loss: 0.020300 [    0/79525]
loss: 0.019009 [ 2560/79525]
loss: 0.019244 [ 5120/79525]
loss: 0.021268 [ 7680/79525]
loss: 0.020462 [10240/79525]
loss: 0.019757 [12800/79525]
loss: 0.021788 [15360/79525]
loss: 0.019226 [17920/79525]
loss: 0.022129 [20480/79525]
loss: 0.020439 [23040/79525]
loss: 0.020208 [25600/79525]
loss: 0.020017 [28160/79525]
loss: 0.020480 [30720/79525]
loss: 0.021820 [33280/79525]
loss: 0.019564 [35840/79525]
loss: 0.020744 [38400/79525]
loss: 0.019246 [40960/79525]
loss: 0.021038 [43520/79525]
loss: 0.020748 [46080/79525]
loss: 0.020148 [48640/79525]
loss: 0.020566 [51200/79525]
loss: 0.019788 [53760/79525]
loss: 0.019359 [56320/79525]
loss: 0.020863 [58880/79525]
loss: 0.019334 [61440/79525]
loss: 0.019876 [64000/79525]
loss: 0.020806 [66560/79525]
loss: 0.019839 [69120/79525]
loss: 0.019014 [71680/79525]
loss: 0.020758 [74240/79525]
loss: 0.020162 [76800/79525]
loss: 0.019776 [51150/79525]
Epoch 4 results: Train loss: 0.020267, Val loss: 0.019624
Validation loss decreased (0.020294 --> 0.019624). Saving model...
Saving best model to best_model.pth
Epoch 5/50
loss: 0.020346 [    0/79525]
loss: 0.020463 [ 2560/79525]
loss: 0.021664 [ 5120/79525]
loss: 0.019500 [ 7680/79525]
loss: 0.020083 [10240/79525]
loss: 0.021146 [12800/79525]
loss: 0.020694 [15360/79525]
loss: 0.019852 [17920/79525]
loss: 0.019475 [20480/79525]
loss: 0.019025 [23040/79525]
loss: 0.020606 [25600/79525]
loss: 0.019606 [28160/79525]
loss: 0.018959 [30720/79525]
loss: 0.019715 [33280/79525]
loss: 0.019410 [35840/79525]
loss: 0.018520 [38400/79525]
loss: 0.019177 [40960/79525]
loss: 0.018959 [43520/79525]
loss: 0.020701 [46080/79525]
loss: 0.020282 [48640/79525]
loss: 0.019674 [51200/79525]
loss: 0.018794 [53760/79525]
loss: 0.019641 [56320/79525]
loss: 0.019915 [58880/79525]
loss: 0.021138 [61440/79525]
loss: 0.020076 [64000/79525]
loss: 0.019213 [66560/79525]
loss: 0.020227 [69120/79525]
loss: 0.019939 [71680/79525]
loss: 0.020006 [74240/79525]
loss: 0.020021 [76800/79525]
loss: 0.020675 [51150/79525]
Epoch 5 results: Train loss: 0.019917, Val loss: 0.019444
Validation loss decreased (0.019624 --> 0.019444). Saving model...
Saving best model to best_model.pth
Epoch 6/50
loss: 0.019379 [    0/79525]
loss: 0.019051 [ 2560/79525]
loss: 0.019109 [ 5120/79525]
loss: 0.020939 [ 7680/79525]
loss: 0.019897 [10240/79525]
loss: 0.020965 [12800/79525]
loss: 0.020326 [15360/79525]
loss: 0.019460 [17920/79525]
loss: 0.019688 [20480/79525]
loss: 0.019506 [23040/79525]
loss: 0.019641 [25600/79525]
loss: 0.020977 [28160/79525]
loss: 0.020037 [30720/79525]
loss: 0.021074 [33280/79525]
loss: 0.020995 [35840/79525]
loss: 0.020092 [38400/79525]
loss: 0.019886 [40960/79525]
loss: 0.020239 [43520/79525]
loss: 0.019333 [46080/79525]
loss: 0.020171 [48640/79525]
loss: 0.019065 [51200/79525]
loss: 0.019097 [53760/79525]
loss: 0.019897 [56320/79525]
loss: 0.019652 [58880/79525]
loss: 0.019632 [61440/79525]
loss: 0.019466 [64000/79525]
loss: 0.021597 [66560/79525]
loss: 0.019460 [69120/79525]
loss: 0.020094 [71680/79525]
loss: 0.020596 [74240/79525]
loss: 0.019555 [76800/79525]
loss: 0.020279 [51150/79525]
Epoch 6 results: Train loss: 0.019955, Val loss: 0.019794
EarlyStopping counter: 1 out of 5
Epoch 7/50
loss: 0.020006 [    0/79525]
loss: 0.021069 [ 2560/79525]
loss: 0.019917 [ 5120/79525]
loss: 0.020616 [ 7680/79525]
loss: 0.021037 [10240/79525]
loss: 0.019788 [12800/79525]
loss: 0.021122 [15360/79525]
loss: 0.020434 [17920/79525]
loss: 0.019846 [20480/79525]
loss: 0.019939 [23040/79525]
loss: 0.020509 [25600/79525]
loss: 0.020930 [28160/79525]
loss: 0.020016 [30720/79525]
loss: 0.019358 [33280/79525]
loss: 0.020255 [35840/79525]
loss: 0.020181 [38400/79525]
loss: 0.019474 [40960/79525]
loss: 0.020927 [43520/79525]
loss: 0.019338 [46080/79525]
loss: 0.020836 [48640/79525]
loss: 0.019906 [51200/79525]
loss: 0.019127 [53760/79525]
loss: 0.021679 [56320/79525]
loss: 0.020479 [58880/79525]
loss: 0.019937 [61440/79525]
loss: 0.021000 [64000/79525]
loss: 0.019809 [66560/79525]
loss: 0.019962 [69120/79525]
loss: 0.019899 [71680/79525]
loss: 0.019515 [74240/79525]
loss: 0.021196 [76800/79525]
loss: 0.018790 [51150/79525]
Epoch 7 results: Train loss: 0.020213, Val loss: 0.019475
EarlyStopping counter: 2 out of 5
Epoch 8/50
loss: 0.021307 [    0/79525]
loss: 0.020259 [ 2560/79525]
loss: 0.019346 [ 5120/79525]
loss: 0.019484 [ 7680/79525]
loss: 0.019628 [10240/79525]
loss: 0.019690 [12800/79525]
loss: 0.017352 [15360/79525]
loss: 0.019426 [17920/79525]
loss: 0.019101 [20480/79525]
loss: 0.019392 [23040/79525]
loss: 0.020495 [25600/79525]
loss: 0.019740 [28160/79525]
loss: 0.020011 [30720/79525]
loss: 0.020584 [33280/79525]
loss: 0.022540 [35840/79525]
loss: 0.020351 [38400/79525]
loss: 0.021131 [40960/79525]
loss: 0.020331 [43520/79525]
loss: 0.020314 [46080/79525]
loss: 0.020761 [48640/79525]
loss: 0.022260 [51200/79525]
loss: 0.021715 [53760/79525]
loss: 0.021760 [56320/79525]
loss: 0.021165 [58880/79525]
loss: 0.021507 [61440/79525]
loss: 0.022466 [64000/79525]
loss: 0.020127 [66560/79525]
loss: 0.021180 [69120/79525]
loss: 0.021121 [71680/79525]
loss: 0.020330 [74240/79525]
loss: 0.021380 [76800/79525]
loss: 0.021441 [51150/79525]
Epoch 8 results: Train loss: 0.020782, Val loss: 0.020544
EarlyStopping counter: 3 out of 5
Epoch 9/50
loss: 0.019501 [    0/79525]
loss: 0.020083 [ 2560/79525]
loss: 0.023450 [ 5120/79525]
loss: 0.021946 [ 7680/79525]
loss: 0.022474 [10240/79525]
loss: 0.020543 [12800/79525]
loss: 0.020341 [15360/79525]
loss: 0.019709 [17920/79525]
loss: 0.021998 [20480/79525]
loss: 0.022367 [23040/79525]
loss: 0.021293 [25600/79525]
loss: 0.029802 [28160/79525]
loss: 0.027500 [30720/79525]
loss: 0.026368 [33280/79525]
loss: 0.025826 [35840/79525]
loss: 0.029335 [38400/79525]
loss: 0.028404 [40960/79525]
loss: 0.029527 [43520/79525]
loss: 0.028074 [46080/79525]
loss: 0.025856 [48640/79525]
loss: 0.024154 [51200/79525]
loss: 0.026698 [53760/79525]
loss: 0.025674 [56320/79525]
loss: 0.026794 [58880/79525]
loss: 0.028300 [61440/79525]
loss: 0.027714 [64000/79525]
loss: 0.025083 [66560/79525]
loss: 0.023872 [69120/79525]
loss: 0.026389 [71680/79525]
loss: 0.023627 [74240/79525]
loss: 0.026201 [76800/79525]
loss: 0.026269 [51150/79525]
Epoch 9 results: Train loss: 0.024716, Val loss: 0.024893
EarlyStopping counter: 4 out of 5
Epoch 10/50
loss: 0.025132 [    0/79525]
loss: 0.024415 [ 2560/79525]
loss: 0.022601 [ 5120/79525]
loss: 0.024755 [ 7680/79525]
loss: 0.022546 [10240/79525]
loss: 0.023618 [12800/79525]
loss: 0.023794 [15360/79525]
loss: 0.023498 [17920/79525]
loss: 0.023316 [20480/79525]
loss: 0.021666 [23040/79525]
loss: 0.023393 [25600/79525]
loss: 0.021360 [28160/79525]
loss: 0.021919 [30720/79525]
loss: 0.024594 [33280/79525]
loss: 0.023368 [35840/79525]
loss: 0.024272 [38400/79525]
loss: 0.022772 [40960/79525]
loss: 0.025093 [43520/79525]
loss: 0.022254 [46080/79525]
loss: 0.022835 [48640/79525]
loss: 0.022721 [51200/79525]
loss: 0.021873 [53760/79525]
loss: 0.022842 [56320/79525]
loss: 0.023664 [58880/79525]
loss: 0.023350 [61440/79525]
loss: 0.022551 [64000/79525]
loss: 0.022878 [66560/79525]
loss: 0.021651 [69120/79525]
loss: 0.022406 [71680/79525]
loss: 0.022382 [74240/79525]
loss: 0.022901 [76800/79525]
loss: 0.021302 [51150/79525]
Epoch 10 results: Train loss: 0.022948, Val loss: 0.021120
EarlyStopping counter: 5 out of 5
Early stopping triggered. Saving final model to final_model.pth
Early stopping triggered
