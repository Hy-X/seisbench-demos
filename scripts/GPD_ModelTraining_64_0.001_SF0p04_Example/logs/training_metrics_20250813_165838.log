2025-08-13 16:58:59,821 - INFO - Epoch 1, Batch 0: loss: 21.025124  [    0/31588]
2025-08-13 16:58:59,971 - INFO - Epoch 1, Batch 20: loss: 16.160673  [ 1280/31588]
2025-08-13 16:59:00,159 - INFO - Epoch 1, Batch 40: loss: 16.588859  [ 2560/31588]
2025-08-13 16:59:00,463 - INFO - Epoch 1, Batch 60: loss: 14.267103  [ 3840/31588]
2025-08-13 16:59:00,753 - INFO - Epoch 1, Batch 80: loss: 14.463587  [ 5120/31588]
2025-08-13 16:59:01,092 - INFO - Epoch 1, Batch 100: loss: 14.693543  [ 6400/31588]
2025-08-13 16:59:01,376 - INFO - Epoch 1, Batch 120: loss: 14.565155  [ 7680/31588]
2025-08-13 16:59:01,608 - INFO - Epoch 1, Batch 140: loss: 14.221647  [ 8960/31588]
2025-08-13 16:59:01,866 - INFO - Epoch 1, Batch 160: loss: 14.408203  [10240/31588]
2025-08-13 16:59:02,196 - INFO - Epoch 1, Batch 180: loss: 13.222016  [11520/31588]
2025-08-13 16:59:02,452 - INFO - Epoch 1, Batch 200: loss: 14.084213  [12800/31588]
2025-08-13 16:59:02,752 - INFO - Epoch 1, Batch 220: loss: 12.652734  [14080/31588]
2025-08-13 16:59:02,998 - INFO - Epoch 1, Batch 240: loss: 13.538805  [15360/31588]
2025-08-13 16:59:03,321 - INFO - Epoch 1, Batch 260: loss: 12.740915  [16640/31588]
2025-08-13 16:59:03,557 - INFO - Epoch 1, Batch 280: loss: 13.487720  [17920/31588]
2025-08-13 16:59:03,767 - INFO - Epoch 1, Batch 300: loss: 12.799039  [19200/31588]
2025-08-13 16:59:04,083 - INFO - Epoch 1, Batch 320: loss: 13.336811  [20480/31588]
2025-08-13 16:59:04,272 - INFO - Epoch 1, Batch 340: loss: 13.773705  [21760/31588]
2025-08-13 16:59:04,576 - INFO - Epoch 1, Batch 360: loss: 12.900118  [23040/31588]
2025-08-13 16:59:04,802 - INFO - Epoch 1, Batch 380: loss: 12.542695  [24320/31588]
2025-08-13 16:59:05,051 - INFO - Epoch 1, Batch 400: loss: 13.169344  [25600/31588]
2025-08-13 16:59:05,228 - INFO - Epoch 1, Batch 420: loss: 13.884735  [26880/31588]
2025-08-13 16:59:05,479 - INFO - Epoch 1, Batch 440: loss: 12.615305  [28160/31588]
2025-08-13 16:59:05,708 - INFO - Epoch 1, Batch 460: loss: 13.555599  [29440/31588]
2025-08-13 16:59:05,962 - INFO - Epoch 1, Batch 480: loss: 14.112448  [30720/31588]
2025-08-13 16:59:06,171 - INFO - Epoch 1 Training Summary:
2025-08-13 16:59:06,171 - INFO -   Average Loss: 13.812409
2025-08-13 16:59:06,171 - INFO -   Accuracy: 0.9046
2025-08-13 16:59:06,171 - INFO -   F1 (macro): 0.9040
2025-08-13 16:59:06,171 - INFO -   Epoch Time: 6.89s
2025-08-13 16:59:06,171 - INFO -   Average Batch Time: 0.0074s
2025-08-13 16:59:06,171 - INFO -   Samples/second: 4581.4
2025-08-13 16:59:07,750 - INFO - Epoch 1 Complete:
2025-08-13 16:59:07,750 - INFO -   Train Loss: 13.812409, Val Loss: 12.914847
2025-08-13 16:59:07,750 - INFO -   Train Acc: 0.9046, Val Acc: 0.9436
2025-08-13 16:59:07,750 - INFO -   Epoch Time: 8.50s
2025-08-13 16:59:07,751 - INFO - Validation loss decreased (inf --> 12.914847). Saving model... (Epoch 1)
2025-08-13 16:59:07,765 - INFO - Saving best model to best_model.pth (Epoch 1)
2025-08-13 16:59:08,001 - INFO - Epoch 2, Batch 0: loss: 12.766819  [    0/31588]
2025-08-13 16:59:08,285 - INFO - Epoch 2, Batch 20: loss: 13.937995  [ 1280/31588]
2025-08-13 16:59:08,565 - INFO - Epoch 2, Batch 40: loss: 12.323661  [ 2560/31588]
2025-08-13 16:59:08,883 - INFO - Epoch 2, Batch 60: loss: 12.203808  [ 3840/31588]
2025-08-13 16:59:09,177 - INFO - Epoch 2, Batch 80: loss: 12.361377  [ 5120/31588]
2025-08-13 16:59:09,455 - INFO - Epoch 2, Batch 100: loss: 12.536676  [ 6400/31588]
2025-08-13 16:59:09,795 - INFO - Epoch 2, Batch 120: loss: 12.564392  [ 7680/31588]
2025-08-13 16:59:10,015 - INFO - Epoch 2, Batch 140: loss: 12.883130  [ 8960/31588]
2025-08-13 16:59:10,332 - INFO - Epoch 2, Batch 160: loss: 14.888733  [10240/31588]
2025-08-13 16:59:10,577 - INFO - Epoch 2, Batch 180: loss: 12.160685  [11520/31588]
2025-08-13 16:59:10,801 - INFO - Epoch 2, Batch 200: loss: 12.297333  [12800/31588]
2025-08-13 16:59:11,056 - INFO - Epoch 2, Batch 220: loss: 12.797113  [14080/31588]
2025-08-13 16:59:11,295 - INFO - Epoch 2, Batch 240: loss: 12.656846  [15360/31588]
2025-08-13 16:59:11,596 - INFO - Epoch 2, Batch 260: loss: 12.578841  [16640/31588]
2025-08-13 16:59:11,851 - INFO - Epoch 2, Batch 280: loss: 12.892533  [17920/31588]
2025-08-13 16:59:12,169 - INFO - Epoch 2, Batch 300: loss: 13.961885  [19200/31588]
2025-08-13 16:59:12,410 - INFO - Epoch 2, Batch 320: loss: 13.572383  [20480/31588]
2025-08-13 16:59:12,628 - INFO - Epoch 2, Batch 340: loss: 12.686167  [21760/31588]
2025-08-13 16:59:13,009 - INFO - Epoch 2, Batch 360: loss: 12.717723  [23040/31588]
2025-08-13 16:59:13,289 - INFO - Epoch 2, Batch 380: loss: 13.339772  [24320/31588]
2025-08-13 16:59:13,669 - INFO - Epoch 2, Batch 400: loss: 12.974273  [25600/31588]
2025-08-13 16:59:13,916 - INFO - Epoch 2, Batch 420: loss: 12.466195  [26880/31588]
2025-08-13 16:59:14,254 - INFO - Epoch 2, Batch 440: loss: 13.759306  [28160/31588]
2025-08-13 16:59:14,458 - INFO - Epoch 2, Batch 460: loss: 12.252974  [29440/31588]
2025-08-13 16:59:14,691 - INFO - Epoch 2, Batch 480: loss: 11.862040  [30720/31588]
2025-08-13 16:59:14,883 - INFO - Epoch 2 Training Summary:
2025-08-13 16:59:14,884 - INFO -   Average Loss: 12.971523
2025-08-13 16:59:14,884 - INFO -   Accuracy: 0.9457
2025-08-13 16:59:14,884 - INFO -   F1 (macro): 0.9456
2025-08-13 16:59:14,884 - INFO -   Epoch Time: 7.06s
2025-08-13 16:59:14,884 - INFO -   Average Batch Time: 0.0073s
2025-08-13 16:59:14,884 - INFO -   Samples/second: 4471.6
2025-08-13 16:59:16,322 - INFO - Epoch 2 Complete:
2025-08-13 16:59:16,322 - INFO -   Train Loss: 12.971523, Val Loss: 12.762888
2025-08-13 16:59:16,322 - INFO -   Train Acc: 0.9457, Val Acc: 0.9523
2025-08-13 16:59:16,322 - INFO -   Epoch Time: 8.55s
2025-08-13 16:59:16,324 - INFO - Validation loss decreased (12.914847 --> 12.762888). Saving model... (Epoch 2)
2025-08-13 16:59:16,341 - INFO - Saving best model to best_model.pth (Epoch 2)
2025-08-13 16:59:16,561 - INFO - Epoch 3, Batch 0: loss: 13.373545  [    0/31588]
2025-08-13 16:59:16,785 - INFO - Epoch 3, Batch 20: loss: 13.038903  [ 1280/31588]
2025-08-13 16:59:17,050 - INFO - Epoch 3, Batch 40: loss: 13.210539  [ 2560/31588]
2025-08-13 16:59:17,356 - INFO - Epoch 3, Batch 60: loss: 12.914812  [ 3840/31588]
2025-08-13 16:59:17,574 - INFO - Epoch 3, Batch 80: loss: 12.558752  [ 5120/31588]
2025-08-13 16:59:17,868 - INFO - Epoch 3, Batch 100: loss: 12.556607  [ 6400/31588]
2025-08-13 16:59:18,130 - INFO - Epoch 3, Batch 120: loss: 12.935722  [ 7680/31588]
2025-08-13 16:59:18,373 - INFO - Epoch 3, Batch 140: loss: 13.094504  [ 8960/31588]
2025-08-13 16:59:18,676 - INFO - Epoch 3, Batch 160: loss: 13.215941  [10240/31588]
2025-08-13 16:59:18,912 - INFO - Epoch 3, Batch 180: loss: 12.849409  [11520/31588]
2025-08-13 16:59:19,228 - INFO - Epoch 3, Batch 200: loss: 13.759092  [12800/31588]
2025-08-13 16:59:19,477 - INFO - Epoch 3, Batch 220: loss: 12.630921  [14080/31588]
2025-08-13 16:59:19,792 - INFO - Epoch 3, Batch 240: loss: 13.229640  [15360/31588]
2025-08-13 16:59:19,975 - INFO - Epoch 3, Batch 260: loss: 12.955105  [16640/31588]
2025-08-13 16:59:20,326 - INFO - Epoch 3, Batch 280: loss: 13.324282  [17920/31588]
2025-08-13 16:59:20,588 - INFO - Epoch 3, Batch 300: loss: 12.515998  [19200/31588]
2025-08-13 16:59:20,879 - INFO - Epoch 3, Batch 320: loss: 12.305562  [20480/31588]
2025-08-13 16:59:21,116 - INFO - Epoch 3, Batch 340: loss: 12.513319  [21760/31588]
2025-08-13 16:59:21,395 - INFO - Epoch 3, Batch 360: loss: 13.446192  [23040/31588]
2025-08-13 16:59:21,645 - INFO - Epoch 3, Batch 380: loss: 13.035599  [24320/31588]
2025-08-13 16:59:21,855 - INFO - Epoch 3, Batch 400: loss: 12.517888  [25600/31588]
2025-08-13 16:59:22,168 - INFO - Epoch 3, Batch 420: loss: 13.126204  [26880/31588]
2025-08-13 16:59:22,421 - INFO - Epoch 3, Batch 440: loss: 13.047964  [28160/31588]
2025-08-13 16:59:22,645 - INFO - Epoch 3, Batch 460: loss: 13.375237  [29440/31588]
2025-08-13 16:59:22,837 - INFO - Epoch 3, Batch 480: loss: 13.011962  [30720/31588]
2025-08-13 16:59:23,033 - INFO - Epoch 3 Training Summary:
2025-08-13 16:59:23,033 - INFO -   Average Loss: 12.830317
2025-08-13 16:59:23,034 - INFO -   Accuracy: 0.9517
2025-08-13 16:59:23,034 - INFO -   F1 (macro): 0.9518
2025-08-13 16:59:23,034 - INFO -   Epoch Time: 6.63s
2025-08-13 16:59:23,034 - INFO -   Average Batch Time: 0.0071s
2025-08-13 16:59:23,034 - INFO -   Samples/second: 4764.2
2025-08-13 16:59:24,678 - INFO - Epoch 3 Complete:
2025-08-13 16:59:24,678 - INFO -   Train Loss: 12.830317, Val Loss: 12.731288
2025-08-13 16:59:24,678 - INFO -   Train Acc: 0.9517, Val Acc: 0.9526
2025-08-13 16:59:24,678 - INFO -   Epoch Time: 8.32s
2025-08-13 16:59:24,679 - INFO - Validation loss decreased (12.762888 --> 12.731288). Saving model... (Epoch 3)
2025-08-13 16:59:24,765 - INFO - Saving best model to best_model.pth (Epoch 3)
2025-08-13 16:59:25,075 - INFO - Epoch 4, Batch 0: loss: 12.980047  [    0/31588]
2025-08-13 16:59:25,313 - INFO - Epoch 4, Batch 20: loss: 13.039840  [ 1280/31588]
2025-08-13 16:59:25,530 - INFO - Epoch 4, Batch 40: loss: 13.079554  [ 2560/31588]
2025-08-13 16:59:25,868 - INFO - Epoch 4, Batch 60: loss: 12.636892  [ 3840/31588]
2025-08-13 16:59:26,081 - INFO - Epoch 4, Batch 80: loss: 12.047611  [ 5120/31588]
2025-08-13 16:59:26,421 - INFO - Epoch 4, Batch 100: loss: 12.973753  [ 6400/31588]
2025-08-13 16:59:26,661 - INFO - Epoch 4, Batch 120: loss: 12.962154  [ 7680/31588]
2025-08-13 16:59:26,923 - INFO - Epoch 4, Batch 140: loss: 12.625108  [ 8960/31588]
2025-08-13 16:59:27,170 - INFO - Epoch 4, Batch 160: loss: 12.664714  [10240/31588]
2025-08-13 16:59:27,478 - INFO - Epoch 4, Batch 180: loss: 12.212251  [11520/31588]
2025-08-13 16:59:27,716 - INFO - Epoch 4, Batch 200: loss: 12.684055  [12800/31588]
2025-08-13 16:59:27,945 - INFO - Epoch 4, Batch 220: loss: 12.209589  [14080/31588]
2025-08-13 16:59:28,206 - INFO - Epoch 4, Batch 240: loss: 12.806629  [15360/31588]
2025-08-13 16:59:28,442 - INFO - Epoch 4, Batch 260: loss: 12.768932  [16640/31588]
2025-08-13 16:59:28,679 - INFO - Epoch 4, Batch 280: loss: 13.680637  [17920/31588]
2025-08-13 16:59:28,927 - INFO - Epoch 4, Batch 300: loss: 12.037089  [19200/31588]
2025-08-13 16:59:29,194 - INFO - Epoch 4, Batch 320: loss: 12.741991  [20480/31588]
2025-08-13 16:59:29,403 - INFO - Epoch 4, Batch 340: loss: 12.789581  [21760/31588]
2025-08-13 16:59:29,764 - INFO - Epoch 4, Batch 360: loss: 13.587844  [23040/31588]
2025-08-13 16:59:30,002 - INFO - Epoch 4, Batch 380: loss: 13.647090  [24320/31588]
2025-08-13 16:59:30,326 - INFO - Epoch 4, Batch 400: loss: 13.583871  [25600/31588]
2025-08-13 16:59:30,528 - INFO - Epoch 4, Batch 420: loss: 12.691135  [26880/31588]
2025-08-13 16:59:30,743 - INFO - Epoch 4, Batch 440: loss: 12.317557  [28160/31588]
2025-08-13 16:59:31,007 - INFO - Epoch 4, Batch 460: loss: 13.399327  [29440/31588]
2025-08-13 16:59:31,230 - INFO - Epoch 4, Batch 480: loss: 12.854175  [30720/31588]
2025-08-13 16:59:31,417 - INFO - Epoch 4 Training Summary:
2025-08-13 16:59:31,418 - INFO -   Average Loss: 12.773114
2025-08-13 16:59:31,418 - INFO -   Accuracy: 0.9547
2025-08-13 16:59:31,418 - INFO -   F1 (macro): 0.9548
2025-08-13 16:59:31,418 - INFO -   Epoch Time: 6.55s
2025-08-13 16:59:31,418 - INFO -   Average Batch Time: 0.0067s
2025-08-13 16:59:31,418 - INFO -   Samples/second: 4823.7
2025-08-13 16:59:32,917 - INFO - Epoch 4 Complete:
2025-08-13 16:59:32,917 - INFO -   Train Loss: 12.773114, Val Loss: 12.679811
2025-08-13 16:59:32,917 - INFO -   Train Acc: 0.9547, Val Acc: 0.9558
2025-08-13 16:59:32,917 - INFO -   Epoch Time: 8.08s
2025-08-13 16:59:32,919 - INFO - Validation loss decreased (12.731288 --> 12.679811). Saving model... (Epoch 4)
2025-08-13 16:59:32,935 - INFO - Saving best model to best_model.pth (Epoch 4)
2025-08-13 16:59:33,151 - INFO - Epoch 5, Batch 0: loss: 12.797160  [    0/31588]
2025-08-13 16:59:33,344 - INFO - Epoch 5, Batch 20: loss: 12.856730  [ 1280/31588]
2025-08-13 16:59:33,584 - INFO - Epoch 5, Batch 40: loss: 12.215357  [ 2560/31588]
2025-08-13 16:59:33,921 - INFO - Epoch 5, Batch 60: loss: 12.701467  [ 3840/31588]
2025-08-13 16:59:34,132 - INFO - Epoch 5, Batch 80: loss: 12.914859  [ 5120/31588]
2025-08-13 16:59:34,472 - INFO - Epoch 5, Batch 100: loss: 13.349412  [ 6400/31588]
2025-08-13 16:59:34,704 - INFO - Epoch 5, Batch 120: loss: 13.442585  [ 7680/31588]
2025-08-13 16:59:35,077 - INFO - Epoch 5, Batch 140: loss: 11.951732  [ 8960/31588]
2025-08-13 16:59:35,283 - INFO - Epoch 5, Batch 160: loss: 12.597170  [10240/31588]
2025-08-13 16:59:35,488 - INFO - Epoch 5, Batch 180: loss: 13.007673  [11520/31588]
2025-08-13 16:59:35,735 - INFO - Epoch 5, Batch 200: loss: 12.419907  [12800/31588]
2025-08-13 16:59:35,948 - INFO - Epoch 5, Batch 220: loss: 12.437150  [14080/31588]
2025-08-13 16:59:36,183 - INFO - Epoch 5, Batch 240: loss: 12.867000  [15360/31588]
2025-08-13 16:59:36,413 - INFO - Epoch 5, Batch 260: loss: 12.879561  [16640/31588]
2025-08-13 16:59:36,717 - INFO - Epoch 5, Batch 280: loss: 14.234659  [17920/31588]
2025-08-13 16:59:36,945 - INFO - Epoch 5, Batch 300: loss: 13.904029  [19200/31588]
2025-08-13 16:59:37,123 - INFO - Epoch 5, Batch 320: loss: 13.269436  [20480/31588]
2025-08-13 16:59:37,375 - INFO - Epoch 5, Batch 340: loss: 13.169114  [21760/31588]
2025-08-13 16:59:37,556 - INFO - Epoch 5, Batch 360: loss: 12.267674  [23040/31588]
2025-08-13 16:59:37,822 - INFO - Epoch 5, Batch 380: loss: 12.667819  [24320/31588]
2025-08-13 16:59:38,027 - INFO - Epoch 5, Batch 400: loss: 12.169684  [25600/31588]
2025-08-13 16:59:38,303 - INFO - Epoch 5, Batch 420: loss: 12.281082  [26880/31588]
2025-08-13 16:59:38,495 - INFO - Epoch 5, Batch 440: loss: 12.905291  [28160/31588]
2025-08-13 16:59:38,692 - INFO - Epoch 5, Batch 460: loss: 13.462403  [29440/31588]
2025-08-13 16:59:38,915 - INFO - Epoch 5, Batch 480: loss: 12.852175  [30720/31588]
2025-08-13 16:59:39,084 - INFO - Epoch 5 Training Summary:
2025-08-13 16:59:39,084 - INFO -   Average Loss: 12.705351
2025-08-13 16:59:39,084 - INFO -   Accuracy: 0.9585
2025-08-13 16:59:39,084 - INFO -   F1 (macro): 0.9585
2025-08-13 16:59:39,084 - INFO -   Epoch Time: 6.09s
2025-08-13 16:59:39,085 - INFO -   Average Batch Time: 0.0073s
2025-08-13 16:59:39,085 - INFO -   Samples/second: 5187.4
2025-08-13 16:59:40,507 - INFO - Epoch 5 Complete:
2025-08-13 16:59:40,507 - INFO -   Train Loss: 12.705351, Val Loss: 12.714209
2025-08-13 16:59:40,507 - INFO -   Train Acc: 0.9585, Val Acc: 0.9546
2025-08-13 16:59:40,507 - INFO -   Epoch Time: 7.56s
2025-08-13 16:59:40,509 - INFO - EarlyStopping counter: 1 out of 5 (Epoch 5)
2025-08-13 16:59:40,732 - INFO - Epoch 6, Batch 0: loss: 12.448477  [    0/31588]
2025-08-13 16:59:40,963 - INFO - Epoch 6, Batch 20: loss: 12.571291  [ 1280/31588]
2025-08-13 16:59:41,164 - INFO - Epoch 6, Batch 40: loss: 11.844015  [ 2560/31588]
2025-08-13 16:59:41,490 - INFO - Epoch 6, Batch 60: loss: 12.741864  [ 3840/31588]
2025-08-13 16:59:41,697 - INFO - Epoch 6, Batch 80: loss: 12.284727  [ 5120/31588]
2025-08-13 16:59:41,958 - INFO - Epoch 6, Batch 100: loss: 13.013057  [ 6400/31588]
2025-08-13 16:59:42,197 - INFO - Epoch 6, Batch 120: loss: 13.464196  [ 7680/31588]
2025-08-13 16:59:42,422 - INFO - Epoch 6, Batch 140: loss: 11.964549  [ 8960/31588]
2025-08-13 16:59:42,702 - INFO - Epoch 6, Batch 160: loss: 11.908483  [10240/31588]
2025-08-13 16:59:42,942 - INFO - Epoch 6, Batch 180: loss: 12.431442  [11520/31588]
2025-08-13 16:59:43,160 - INFO - Epoch 6, Batch 200: loss: 13.087934  [12800/31588]
2025-08-13 16:59:43,419 - INFO - Epoch 6, Batch 220: loss: 12.355812  [14080/31588]
2025-08-13 16:59:43,695 - INFO - Epoch 6, Batch 240: loss: 13.466258  [15360/31588]
2025-08-13 16:59:43,910 - INFO - Epoch 6, Batch 260: loss: 12.295166  [16640/31588]
2025-08-13 16:59:44,209 - INFO - Epoch 6, Batch 280: loss: 12.383436  [17920/31588]
2025-08-13 16:59:44,396 - INFO - Epoch 6, Batch 300: loss: 12.439016  [19200/31588]
2025-08-13 16:59:44,657 - INFO - Epoch 6, Batch 320: loss: 13.959038  [20480/31588]
2025-08-13 16:59:44,863 - INFO - Epoch 6, Batch 340: loss: 12.705857  [21760/31588]
2025-08-13 16:59:45,191 - INFO - Epoch 6, Batch 360: loss: 12.260538  [23040/31588]
2025-08-13 16:59:45,453 - INFO - Epoch 6, Batch 380: loss: 12.729098  [24320/31588]
2025-08-13 16:59:45,675 - INFO - Epoch 6, Batch 400: loss: 12.298000  [25600/31588]
2025-08-13 16:59:46,008 - INFO - Epoch 6, Batch 420: loss: 13.614373  [26880/31588]
2025-08-13 16:59:46,200 - INFO - Epoch 6, Batch 440: loss: 13.306788  [28160/31588]
2025-08-13 16:59:46,410 - INFO - Epoch 6, Batch 460: loss: 13.485460  [29440/31588]
2025-08-13 16:59:46,645 - INFO - Epoch 6, Batch 480: loss: 13.159393  [30720/31588]
2025-08-13 16:59:46,870 - INFO - Epoch 6 Training Summary:
2025-08-13 16:59:46,870 - INFO -   Average Loss: 12.671958
2025-08-13 16:59:46,870 - INFO -   Accuracy: 0.9599
2025-08-13 16:59:46,870 - INFO -   F1 (macro): 0.9599
2025-08-13 16:59:46,870 - INFO -   Epoch Time: 6.32s
2025-08-13 16:59:46,870 - INFO -   Average Batch Time: 0.0068s
2025-08-13 16:59:46,870 - INFO -   Samples/second: 4995.9
2025-08-13 16:59:48,328 - INFO - Epoch 6 Complete:
2025-08-13 16:59:48,328 - INFO -   Train Loss: 12.671958, Val Loss: 12.620105
2025-08-13 16:59:48,328 - INFO -   Train Acc: 0.9599, Val Acc: 0.9589
2025-08-13 16:59:48,328 - INFO -   Epoch Time: 7.82s
2025-08-13 16:59:48,330 - INFO - Validation loss decreased (12.679811 --> 12.620105). Saving model... (Epoch 6)
2025-08-13 16:59:48,347 - INFO - Saving best model to best_model.pth (Epoch 6)
2025-08-13 16:59:48,555 - INFO - Epoch 7, Batch 0: loss: 12.982876  [    0/31588]
2025-08-13 16:59:48,778 - INFO - Epoch 7, Batch 20: loss: 12.804304  [ 1280/31588]
2025-08-13 16:59:49,014 - INFO - Epoch 7, Batch 40: loss: 12.554343  [ 2560/31588]
2025-08-13 16:59:49,312 - INFO - Epoch 7, Batch 60: loss: 12.589709  [ 3840/31588]
2025-08-13 16:59:49,548 - INFO - Epoch 7, Batch 80: loss: 12.955762  [ 5120/31588]
2025-08-13 16:59:49,751 - INFO - Epoch 7, Batch 100: loss: 12.840875  [ 6400/31588]
2025-08-13 16:59:50,088 - INFO - Epoch 7, Batch 120: loss: 12.983171  [ 7680/31588]
2025-08-13 16:59:50,330 - INFO - Epoch 7, Batch 140: loss: 13.643832  [ 8960/31588]
2025-08-13 16:59:50,621 - INFO - Epoch 7, Batch 160: loss: 12.646738  [10240/31588]
2025-08-13 16:59:50,816 - INFO - Epoch 7, Batch 180: loss: 12.354890  [11520/31588]
2025-08-13 16:59:51,034 - INFO - Epoch 7, Batch 200: loss: 12.169493  [12800/31588]
2025-08-13 16:59:51,332 - INFO - Epoch 7, Batch 220: loss: 12.742765  [14080/31588]
2025-08-13 16:59:51,770 - INFO - Epoch 7, Batch 240: loss: 12.389876  [15360/31588]
2025-08-13 16:59:52,062 - INFO - Epoch 7, Batch 260: loss: 11.834248  [16640/31588]
2025-08-13 16:59:52,320 - INFO - Epoch 7, Batch 280: loss: 12.367253  [17920/31588]
2025-08-13 16:59:52,648 - INFO - Epoch 7, Batch 300: loss: 12.950159  [19200/31588]
2025-08-13 16:59:52,931 - INFO - Epoch 7, Batch 320: loss: 12.331229  [20480/31588]
2025-08-13 16:59:53,317 - INFO - Epoch 7, Batch 340: loss: 12.203022  [21760/31588]
2025-08-13 16:59:53,548 - INFO - Epoch 7, Batch 360: loss: 12.903044  [23040/31588]
2025-08-13 16:59:53,829 - INFO - Epoch 7, Batch 380: loss: 11.916673  [24320/31588]
2025-08-13 16:59:54,022 - INFO - Epoch 7, Batch 400: loss: 12.237401  [25600/31588]
2025-08-13 16:59:54,223 - INFO - Epoch 7, Batch 420: loss: 12.766908  [26880/31588]
2025-08-13 16:59:54,478 - INFO - Epoch 7, Batch 440: loss: 12.180596  [28160/31588]
2025-08-13 16:59:54,634 - INFO - Epoch 7, Batch 460: loss: 12.708682  [29440/31588]
2025-08-13 16:59:54,890 - INFO - Epoch 7, Batch 480: loss: 12.868076  [30720/31588]
2025-08-13 16:59:55,067 - INFO - Epoch 7 Training Summary:
2025-08-13 16:59:55,067 - INFO -   Average Loss: 12.613343
2025-08-13 16:59:55,067 - INFO -   Accuracy: 0.9624
2025-08-13 16:59:55,067 - INFO -   F1 (macro): 0.9625
2025-08-13 16:59:55,067 - INFO -   Epoch Time: 6.68s
2025-08-13 16:59:55,067 - INFO -   Average Batch Time: 0.0068s
2025-08-13 16:59:55,067 - INFO -   Samples/second: 4728.7
2025-08-13 16:59:56,420 - INFO - Epoch 7 Complete:
2025-08-13 16:59:56,420 - INFO -   Train Loss: 12.613343, Val Loss: 12.687414
2025-08-13 16:59:56,420 - INFO -   Train Acc: 0.9624, Val Acc: 0.9560
2025-08-13 16:59:56,420 - INFO -   Epoch Time: 8.06s
2025-08-13 16:59:56,421 - INFO - EarlyStopping counter: 1 out of 5 (Epoch 7)
2025-08-13 16:59:56,599 - INFO - Epoch 8, Batch 0: loss: 12.736650  [    0/31588]
2025-08-13 16:59:56,834 - INFO - Epoch 8, Batch 20: loss: 12.352550  [ 1280/31588]
2025-08-13 16:59:57,055 - INFO - Epoch 8, Batch 40: loss: 12.902337  [ 2560/31588]
2025-08-13 16:59:57,352 - INFO - Epoch 8, Batch 60: loss: 12.173420  [ 3840/31588]
2025-08-13 16:59:57,594 - INFO - Epoch 8, Batch 80: loss: 11.830553  [ 5120/31588]
2025-08-13 16:59:57,840 - INFO - Epoch 8, Batch 100: loss: 12.596247  [ 6400/31588]
2025-08-13 16:59:58,091 - INFO - Epoch 8, Batch 120: loss: 13.163277  [ 7680/31588]
2025-08-13 16:59:58,309 - INFO - Epoch 8, Batch 140: loss: 12.968859  [ 8960/31588]
2025-08-13 16:59:58,655 - INFO - Epoch 8, Batch 160: loss: 13.759823  [10240/31588]
2025-08-13 16:59:58,849 - INFO - Epoch 8, Batch 180: loss: 12.653846  [11520/31588]
2025-08-13 16:59:59,108 - INFO - Epoch 8, Batch 200: loss: 11.899254  [12800/31588]
2025-08-13 16:59:59,369 - INFO - Epoch 8, Batch 220: loss: 12.606863  [14080/31588]
2025-08-13 16:59:59,578 - INFO - Epoch 8, Batch 240: loss: 12.225327  [15360/31588]
2025-08-13 16:59:59,820 - INFO - Epoch 8, Batch 260: loss: 12.746338  [16640/31588]
2025-08-13 17:00:00,024 - INFO - Epoch 8, Batch 280: loss: 11.859119  [17920/31588]
2025-08-13 17:00:00,291 - INFO - Epoch 8, Batch 300: loss: 12.792846  [19200/31588]
2025-08-13 17:00:00,493 - INFO - Epoch 8, Batch 320: loss: 13.175341  [20480/31588]
2025-08-13 17:00:00,731 - INFO - Epoch 8, Batch 340: loss: 12.085944  [21760/31588]
2025-08-13 17:00:00,963 - INFO - Epoch 8, Batch 360: loss: 12.452882  [23040/31588]
2025-08-13 17:00:01,154 - INFO - Epoch 8, Batch 380: loss: 12.357017  [24320/31588]
2025-08-13 17:00:01,474 - INFO - Epoch 8, Batch 400: loss: 12.784142  [25600/31588]
2025-08-13 17:00:01,691 - INFO - Epoch 8, Batch 420: loss: 13.322580  [26880/31588]
2025-08-13 17:00:01,950 - INFO - Epoch 8, Batch 440: loss: 13.191032  [28160/31588]
2025-08-13 17:00:02,139 - INFO - Epoch 8, Batch 460: loss: 12.807254  [29440/31588]
2025-08-13 17:00:02,315 - INFO - Epoch 8, Batch 480: loss: 11.837644  [30720/31588]
2025-08-13 17:00:02,481 - INFO - Epoch 8 Training Summary:
2025-08-13 17:00:02,481 - INFO -   Average Loss: 12.603162
2025-08-13 17:00:02,481 - INFO -   Accuracy: 0.9628
2025-08-13 17:00:02,481 - INFO -   F1 (macro): 0.9629
2025-08-13 17:00:02,481 - INFO -   Epoch Time: 6.03s
2025-08-13 17:00:02,481 - INFO -   Average Batch Time: 0.0070s
2025-08-13 17:00:02,481 - INFO -   Samples/second: 5235.1
2025-08-13 17:00:04,089 - INFO - Epoch 8 Complete:
2025-08-13 17:00:04,089 - INFO -   Train Loss: 12.603162, Val Loss: 12.640229
2025-08-13 17:00:04,089 - INFO -   Train Acc: 0.9628, Val Acc: 0.9571
2025-08-13 17:00:04,090 - INFO -   Epoch Time: 7.67s
2025-08-13 17:00:04,091 - INFO - EarlyStopping counter: 2 out of 5 (Epoch 8)
2025-08-13 17:00:04,279 - INFO - Epoch 9, Batch 0: loss: 12.554865  [    0/31588]
2025-08-13 17:00:04,566 - INFO - Epoch 9, Batch 20: loss: 12.594085  [ 1280/31588]
2025-08-13 17:00:04,786 - INFO - Epoch 9, Batch 40: loss: 12.326004  [ 2560/31588]
2025-08-13 17:00:05,142 - INFO - Epoch 9, Batch 60: loss: 12.662012  [ 3840/31588]
2025-08-13 17:00:05,356 - INFO - Epoch 9, Batch 80: loss: 13.152574  [ 5120/31588]
2025-08-13 17:00:05,643 - INFO - Epoch 9, Batch 100: loss: 12.841243  [ 6400/31588]
2025-08-13 17:00:05,858 - INFO - Epoch 9, Batch 120: loss: 13.667729  [ 7680/31588]
2025-08-13 17:00:06,044 - INFO - Epoch 9, Batch 140: loss: 13.070589  [ 8960/31588]
2025-08-13 17:00:06,391 - INFO - Epoch 9, Batch 160: loss: 12.157849  [10240/31588]
2025-08-13 17:00:06,629 - INFO - Epoch 9, Batch 180: loss: 12.150669  [11520/31588]
2025-08-13 17:00:06,999 - INFO - Epoch 9, Batch 200: loss: 13.166106  [12800/31588]
2025-08-13 17:00:07,224 - INFO - Epoch 9, Batch 220: loss: 12.563365  [14080/31588]
2025-08-13 17:00:07,518 - INFO - Epoch 9, Batch 240: loss: 13.603672  [15360/31588]
2025-08-13 17:00:07,717 - INFO - Epoch 9, Batch 260: loss: 12.202317  [16640/31588]
2025-08-13 17:00:07,953 - INFO - Epoch 9, Batch 280: loss: 12.244772  [17920/31588]
2025-08-13 17:00:08,273 - INFO - Epoch 9, Batch 300: loss: 12.834710  [19200/31588]
2025-08-13 17:00:08,503 - INFO - Epoch 9, Batch 320: loss: 11.852488  [20480/31588]
2025-08-13 17:00:08,898 - INFO - Epoch 9, Batch 340: loss: 12.170771  [21760/31588]
2025-08-13 17:00:09,133 - INFO - Epoch 9, Batch 360: loss: 13.363879  [23040/31588]
2025-08-13 17:00:09,562 - INFO - Epoch 9, Batch 380: loss: 12.274981  [24320/31588]
2025-08-13 17:00:09,785 - INFO - Epoch 9, Batch 400: loss: 12.801903  [25600/31588]
2025-08-13 17:00:10,003 - INFO - Epoch 9, Batch 420: loss: 13.061097  [26880/31588]
2025-08-13 17:00:10,337 - INFO - Epoch 9, Batch 440: loss: 12.780511  [28160/31588]
2025-08-13 17:00:10,530 - INFO - Epoch 9, Batch 460: loss: 12.881204  [29440/31588]
2025-08-13 17:00:10,813 - INFO - Epoch 9, Batch 480: loss: 12.833496  [30720/31588]
2025-08-13 17:00:10,972 - INFO - Epoch 9 Training Summary:
2025-08-13 17:00:10,972 - INFO -   Average Loss: 12.556807
2025-08-13 17:00:10,972 - INFO -   Accuracy: 0.9653
2025-08-13 17:00:10,972 - INFO -   F1 (macro): 0.9654
2025-08-13 17:00:10,972 - INFO -   Epoch Time: 6.86s
2025-08-13 17:00:10,972 - INFO -   Average Batch Time: 0.0053s
2025-08-13 17:00:10,972 - INFO -   Samples/second: 4607.6
2025-08-13 17:00:12,393 - INFO - Epoch 9 Complete:
2025-08-13 17:00:12,393 - INFO -   Train Loss: 12.556807, Val Loss: 12.601312
2025-08-13 17:00:12,393 - INFO -   Train Acc: 0.9653, Val Acc: 0.9598
2025-08-13 17:00:12,393 - INFO -   Epoch Time: 8.30s
2025-08-13 17:00:12,394 - INFO - Validation loss decreased (12.620105 --> 12.601312). Saving model... (Epoch 9)
2025-08-13 17:00:12,404 - INFO - Saving best model to best_model.pth (Epoch 9)
2025-08-13 17:00:12,560 - INFO - Epoch 10, Batch 0: loss: 13.238200  [    0/31588]
2025-08-13 17:00:12,807 - INFO - Epoch 10, Batch 20: loss: 13.158031  [ 1280/31588]
2025-08-13 17:00:12,999 - INFO - Epoch 10, Batch 40: loss: 12.439383  [ 2560/31588]
2025-08-13 17:00:13,274 - INFO - Epoch 10, Batch 60: loss: 12.799053  [ 3840/31588]
2025-08-13 17:00:13,506 - INFO - Epoch 10, Batch 80: loss: 12.132128  [ 5120/31588]
2025-08-13 17:00:13,744 - INFO - Epoch 10, Batch 100: loss: 12.260252  [ 6400/31588]
2025-08-13 17:00:14,098 - INFO - Epoch 10, Batch 120: loss: 12.196353  [ 7680/31588]
2025-08-13 17:00:14,329 - INFO - Epoch 10, Batch 140: loss: 12.672444  [ 8960/31588]
2025-08-13 17:00:14,683 - INFO - Epoch 10, Batch 160: loss: 12.276532  [10240/31588]
2025-08-13 17:00:14,874 - INFO - Epoch 10, Batch 180: loss: 11.856498  [11520/31588]
2025-08-13 17:00:15,066 - INFO - Epoch 10, Batch 200: loss: 12.258737  [12800/31588]
2025-08-13 17:00:15,400 - INFO - Epoch 10, Batch 220: loss: 13.491116  [14080/31588]
2025-08-13 17:00:15,606 - INFO - Epoch 10, Batch 240: loss: 12.446127  [15360/31588]
2025-08-13 17:00:15,868 - INFO - Epoch 10, Batch 260: loss: 11.901120  [16640/31588]
2025-08-13 17:00:16,084 - INFO - Epoch 10, Batch 280: loss: 11.850398  [17920/31588]
2025-08-13 17:00:16,356 - INFO - Epoch 10, Batch 300: loss: 12.835623  [19200/31588]
2025-08-13 17:00:16,568 - INFO - Epoch 10, Batch 320: loss: 13.164939  [20480/31588]
2025-08-13 17:00:16,781 - INFO - Epoch 10, Batch 340: loss: 12.231953  [21760/31588]
2025-08-13 17:00:17,044 - INFO - Epoch 10, Batch 360: loss: 11.824345  [23040/31588]
2025-08-13 17:00:17,238 - INFO - Epoch 10, Batch 380: loss: 11.959437  [24320/31588]
2025-08-13 17:00:17,505 - INFO - Epoch 10, Batch 400: loss: 12.546562  [25600/31588]
2025-08-13 17:00:17,742 - INFO - Epoch 10, Batch 420: loss: 12.189819  [26880/31588]
2025-08-13 17:00:18,040 - INFO - Epoch 10, Batch 440: loss: 12.688984  [28160/31588]
2025-08-13 17:00:18,217 - INFO - Epoch 10, Batch 460: loss: 12.472629  [29440/31588]
2025-08-13 17:00:18,415 - INFO - Epoch 10, Batch 480: loss: 13.183753  [30720/31588]
2025-08-13 17:00:18,609 - INFO - Epoch 10 Training Summary:
2025-08-13 17:00:18,609 - INFO -   Average Loss: 12.526680
2025-08-13 17:00:18,609 - INFO -   Accuracy: 0.9667
2025-08-13 17:00:18,609 - INFO -   F1 (macro): 0.9667
2025-08-13 17:00:18,609 - INFO -   Epoch Time: 6.15s
2025-08-13 17:00:18,609 - INFO -   Average Batch Time: 0.0065s
2025-08-13 17:00:18,609 - INFO -   Samples/second: 5134.5
2025-08-13 17:00:20,258 - INFO - Epoch 10 Complete:
2025-08-13 17:00:20,258 - INFO -   Train Loss: 12.526680, Val Loss: 12.628789
2025-08-13 17:00:20,258 - INFO -   Train Acc: 0.9667, Val Acc: 0.9584
2025-08-13 17:00:20,258 - INFO -   Epoch Time: 7.85s
2025-08-13 17:00:20,259 - INFO - EarlyStopping counter: 1 out of 5 (Epoch 10)
2025-08-13 17:00:20,484 - INFO - Epoch 11, Batch 0: loss: 12.575799  [    0/31588]
2025-08-13 17:00:20,737 - INFO - Epoch 11, Batch 20: loss: 12.188187  [ 1280/31588]
2025-08-13 17:00:20,988 - INFO - Epoch 11, Batch 40: loss: 12.208233  [ 2560/31588]
2025-08-13 17:00:21,321 - INFO - Epoch 11, Batch 60: loss: 12.811069  [ 3840/31588]
2025-08-13 17:00:21,536 - INFO - Epoch 11, Batch 80: loss: 12.537046  [ 5120/31588]
2025-08-13 17:00:21,846 - INFO - Epoch 11, Batch 100: loss: 12.217001  [ 6400/31588]
2025-08-13 17:00:22,074 - INFO - Epoch 11, Batch 120: loss: 12.323077  [ 7680/31588]
2025-08-13 17:00:22,321 - INFO - Epoch 11, Batch 140: loss: 12.835585  [ 8960/31588]
2025-08-13 17:00:22,638 - INFO - Epoch 11, Batch 160: loss: 12.119092  [10240/31588]
2025-08-13 17:00:22,836 - INFO - Epoch 11, Batch 180: loss: 12.502820  [11520/31588]
2025-08-13 17:00:23,165 - INFO - Epoch 11, Batch 200: loss: 11.878414  [12800/31588]
2025-08-13 17:00:23,416 - INFO - Epoch 11, Batch 220: loss: 12.046251  [14080/31588]
2025-08-13 17:00:23,723 - INFO - Epoch 11, Batch 240: loss: 12.453004  [15360/31588]
2025-08-13 17:00:23,952 - INFO - Epoch 11, Batch 260: loss: 12.769720  [16640/31588]
2025-08-13 17:00:24,161 - INFO - Epoch 11, Batch 280: loss: 12.817515  [17920/31588]
2025-08-13 17:00:24,420 - INFO - Epoch 11, Batch 300: loss: 12.768698  [19200/31588]
2025-08-13 17:00:24,671 - INFO - Epoch 11, Batch 320: loss: 12.596086  [20480/31588]
2025-08-13 17:00:24,916 - INFO - Epoch 11, Batch 340: loss: 11.841901  [21760/31588]
2025-08-13 17:00:25,171 - INFO - Epoch 11, Batch 360: loss: 12.492293  [23040/31588]
2025-08-13 17:00:25,447 - INFO - Epoch 11, Batch 380: loss: 12.247853  [24320/31588]
2025-08-13 17:00:25,648 - INFO - Epoch 11, Batch 400: loss: 12.405274  [25600/31588]
2025-08-13 17:00:25,891 - INFO - Epoch 11, Batch 420: loss: 12.803623  [26880/31588]
2025-08-13 17:00:26,201 - INFO - Epoch 11, Batch 440: loss: 12.467196  [28160/31588]
2025-08-13 17:00:26,420 - INFO - Epoch 11, Batch 460: loss: 13.100220  [29440/31588]
2025-08-13 17:00:26,725 - INFO - Epoch 11, Batch 480: loss: 12.526598  [30720/31588]
2025-08-13 17:00:26,911 - INFO - Epoch 11 Training Summary:
2025-08-13 17:00:26,911 - INFO -   Average Loss: 12.500901
2025-08-13 17:00:26,911 - INFO -   Accuracy: 0.9680
2025-08-13 17:00:26,911 - INFO -   F1 (macro): 0.9681
2025-08-13 17:00:26,911 - INFO -   Epoch Time: 6.58s
2025-08-13 17:00:26,911 - INFO -   Average Batch Time: 0.0075s
2025-08-13 17:00:26,911 - INFO -   Samples/second: 4798.4
2025-08-13 17:00:28,449 - INFO - Epoch 11 Complete:
2025-08-13 17:00:28,449 - INFO -   Train Loss: 12.500901, Val Loss: 12.607048
2025-08-13 17:00:28,449 - INFO -   Train Acc: 0.9680, Val Acc: 0.9590
2025-08-13 17:00:28,449 - INFO -   Epoch Time: 8.16s
2025-08-13 17:00:28,450 - INFO - EarlyStopping counter: 2 out of 5 (Epoch 11)
2025-08-13 17:00:28,619 - INFO - Epoch 12, Batch 0: loss: 12.486962  [    0/31588]
2025-08-13 17:00:28,928 - INFO - Epoch 12, Batch 20: loss: 12.993979  [ 1280/31588]
2025-08-13 17:00:29,174 - INFO - Epoch 12, Batch 40: loss: 12.231161  [ 2560/31588]
2025-08-13 17:00:29,483 - INFO - Epoch 12, Batch 60: loss: 13.388922  [ 3840/31588]
2025-08-13 17:00:29,750 - INFO - Epoch 12, Batch 80: loss: 12.681725  [ 5120/31588]
2025-08-13 17:00:30,048 - INFO - Epoch 12, Batch 100: loss: 12.221364  [ 6400/31588]
2025-08-13 17:00:30,297 - INFO - Epoch 12, Batch 120: loss: 12.131612  [ 7680/31588]
2025-08-13 17:00:30,552 - INFO - Epoch 12, Batch 140: loss: 12.294881  [ 8960/31588]
2025-08-13 17:00:30,823 - INFO - Epoch 12, Batch 160: loss: 12.501954  [10240/31588]
2025-08-13 17:00:31,052 - INFO - Epoch 12, Batch 180: loss: 11.941977  [11520/31588]
2025-08-13 17:00:31,401 - INFO - Epoch 12, Batch 200: loss: 13.463104  [12800/31588]
2025-08-13 17:00:31,619 - INFO - Epoch 12, Batch 220: loss: 12.448604  [14080/31588]
2025-08-13 17:00:31,919 - INFO - Epoch 12, Batch 240: loss: 11.853160  [15360/31588]
2025-08-13 17:00:32,182 - INFO - Epoch 12, Batch 260: loss: 12.872607  [16640/31588]
2025-08-13 17:00:32,434 - INFO - Epoch 12, Batch 280: loss: 12.561761  [17920/31588]
2025-08-13 17:00:32,754 - INFO - Epoch 12, Batch 300: loss: 12.150373  [19200/31588]
2025-08-13 17:00:33,021 - INFO - Epoch 12, Batch 320: loss: 12.886150  [20480/31588]
2025-08-13 17:00:33,257 - INFO - Epoch 12, Batch 340: loss: 12.549133  [21760/31588]
2025-08-13 17:00:33,544 - INFO - Epoch 12, Batch 360: loss: 12.244054  [23040/31588]
2025-08-13 17:00:33,770 - INFO - Epoch 12, Batch 380: loss: 11.860408  [24320/31588]
2025-08-13 17:00:34,110 - INFO - Epoch 12, Batch 400: loss: 12.530837  [25600/31588]
2025-08-13 17:00:34,372 - INFO - Epoch 12, Batch 420: loss: 14.555877  [26880/31588]
2025-08-13 17:00:34,624 - INFO - Epoch 12, Batch 440: loss: 12.140851  [28160/31588]
2025-08-13 17:00:34,920 - INFO - Epoch 12, Batch 460: loss: 12.166030  [29440/31588]
2025-08-13 17:00:35,172 - INFO - Epoch 12, Batch 480: loss: 12.402219  [30720/31588]
2025-08-13 17:00:35,408 - INFO - Epoch 12 Training Summary:
2025-08-13 17:00:35,408 - INFO -   Average Loss: 12.472914
2025-08-13 17:00:35,408 - INFO -   Accuracy: 0.9694
2025-08-13 17:00:35,408 - INFO -   F1 (macro): 0.9695
2025-08-13 17:00:35,408 - INFO -   Epoch Time: 6.91s
2025-08-13 17:00:35,408 - INFO -   Average Batch Time: 0.0076s
2025-08-13 17:00:35,408 - INFO -   Samples/second: 4572.4
2025-08-13 17:00:37,061 - INFO - Epoch 12 Complete:
2025-08-13 17:00:37,061 - INFO -   Train Loss: 12.472914, Val Loss: 12.631329
2025-08-13 17:00:37,061 - INFO -   Train Acc: 0.9694, Val Acc: 0.9589
2025-08-13 17:00:37,061 - INFO -   Epoch Time: 8.61s
2025-08-13 17:00:37,063 - INFO - EarlyStopping counter: 3 out of 5 (Epoch 12)
2025-08-13 17:00:37,273 - INFO - Epoch 13, Batch 0: loss: 11.935548  [    0/31588]
2025-08-13 17:00:37,547 - INFO - Epoch 13, Batch 20: loss: 12.827378  [ 1280/31588]
2025-08-13 17:00:37,781 - INFO - Epoch 13, Batch 40: loss: 13.264787  [ 2560/31588]
2025-08-13 17:00:38,087 - INFO - Epoch 13, Batch 60: loss: 11.852225  [ 3840/31588]
2025-08-13 17:00:38,303 - INFO - Epoch 13, Batch 80: loss: 11.844091  [ 5120/31588]
2025-08-13 17:00:38,632 - INFO - Epoch 13, Batch 100: loss: 12.021079  [ 6400/31588]
2025-08-13 17:00:38,893 - INFO - Epoch 13, Batch 120: loss: 12.465199  [ 7680/31588]
2025-08-13 17:00:39,158 - INFO - Epoch 13, Batch 140: loss: 12.978038  [ 8960/31588]
2025-08-13 17:00:39,448 - INFO - Epoch 13, Batch 160: loss: 12.409462  [10240/31588]
2025-08-13 17:00:39,690 - INFO - Epoch 13, Batch 180: loss: 11.944731  [11520/31588]
2025-08-13 17:00:39,906 - INFO - Epoch 13, Batch 200: loss: 13.163710  [12800/31588]
2025-08-13 17:00:40,089 - INFO - Epoch 13, Batch 220: loss: 12.532585  [14080/31588]
2025-08-13 17:00:40,377 - INFO - Epoch 13, Batch 240: loss: 12.557711  [15360/31588]
2025-08-13 17:00:40,597 - INFO - Epoch 13, Batch 260: loss: 12.011888  [16640/31588]
2025-08-13 17:00:40,790 - INFO - Epoch 13, Batch 280: loss: 12.608187  [17920/31588]
2025-08-13 17:00:41,047 - INFO - Epoch 13, Batch 300: loss: 14.068845  [19200/31588]
2025-08-13 17:00:41,342 - INFO - Epoch 13, Batch 320: loss: 12.088072  [20480/31588]
2025-08-13 17:00:41,568 - INFO - Epoch 13, Batch 340: loss: 12.309945  [21760/31588]
2025-08-13 17:00:41,868 - INFO - Epoch 13, Batch 360: loss: 12.517798  [23040/31588]
2025-08-13 17:00:42,085 - INFO - Epoch 13, Batch 380: loss: 12.035499  [24320/31588]
2025-08-13 17:00:42,279 - INFO - Epoch 13, Batch 400: loss: 12.595251  [25600/31588]
2025-08-13 17:00:42,535 - INFO - Epoch 13, Batch 420: loss: 12.484355  [26880/31588]
2025-08-13 17:00:42,715 - INFO - Epoch 13, Batch 440: loss: 13.581720  [28160/31588]
2025-08-13 17:00:42,963 - INFO - Epoch 13, Batch 460: loss: 12.176391  [29440/31588]
2025-08-13 17:00:43,111 - INFO - Epoch 13, Batch 480: loss: 12.194671  [30720/31588]
2025-08-13 17:00:43,267 - INFO - Epoch 13 Training Summary:
2025-08-13 17:00:43,267 - INFO -   Average Loss: 12.465533
2025-08-13 17:00:43,268 - INFO -   Accuracy: 0.9691
2025-08-13 17:00:43,268 - INFO -   F1 (macro): 0.9691
2025-08-13 17:00:43,268 - INFO -   Epoch Time: 6.16s
2025-08-13 17:00:43,268 - INFO -   Average Batch Time: 0.0066s
2025-08-13 17:00:43,269 - INFO -   Samples/second: 5124.6
2025-08-13 17:00:44,845 - INFO - Epoch 13 Complete:
2025-08-13 17:00:44,986 - INFO -   Train Loss: 12.465533, Val Loss: 12.577297
2025-08-13 17:00:44,986 - INFO -   Train Acc: 0.9691, Val Acc: 0.9605
2025-08-13 17:00:44,986 - INFO -   Epoch Time: 7.78s
2025-08-13 17:00:44,988 - INFO - Validation loss decreased (12.601312 --> 12.577297). Saving model... (Epoch 13)
2025-08-13 17:00:45,002 - INFO - Saving best model to best_model.pth (Epoch 13)
2025-08-13 17:00:45,185 - INFO - Epoch 14, Batch 0: loss: 12.134212  [    0/31588]
2025-08-13 17:00:45,461 - INFO - Epoch 14, Batch 20: loss: 12.816730  [ 1280/31588]
2025-08-13 17:00:45,688 - INFO - Epoch 14, Batch 40: loss: 12.538515  [ 2560/31588]
2025-08-13 17:00:45,954 - INFO - Epoch 14, Batch 60: loss: 13.252605  [ 3840/31588]
2025-08-13 17:00:46,279 - INFO - Epoch 14, Batch 80: loss: 12.377542  [ 5120/31588]
2025-08-13 17:00:46,471 - INFO - Epoch 14, Batch 100: loss: 12.017627  [ 6400/31588]
2025-08-13 17:00:46,732 - INFO - Epoch 14, Batch 120: loss: 12.181665  [ 7680/31588]
2025-08-13 17:00:46,938 - INFO - Epoch 14, Batch 140: loss: 12.223894  [ 8960/31588]
2025-08-13 17:00:47,277 - INFO - Epoch 14, Batch 160: loss: 12.326118  [10240/31588]
2025-08-13 17:00:47,515 - INFO - Epoch 14, Batch 180: loss: 12.401079  [11520/31588]
2025-08-13 17:00:47,842 - INFO - Epoch 14, Batch 200: loss: 11.866573  [12800/31588]
2025-08-13 17:00:48,062 - INFO - Epoch 14, Batch 220: loss: 12.008371  [14080/31588]
2025-08-13 17:00:48,282 - INFO - Epoch 14, Batch 240: loss: 13.135737  [15360/31588]
2025-08-13 17:00:48,604 - INFO - Epoch 14, Batch 260: loss: 12.294665  [16640/31588]
2025-08-13 17:00:48,826 - INFO - Epoch 14, Batch 280: loss: 13.191003  [17920/31588]
2025-08-13 17:00:49,120 - INFO - Epoch 14, Batch 300: loss: 13.633246  [19200/31588]
2025-08-13 17:00:49,332 - INFO - Epoch 14, Batch 320: loss: 12.460888  [20480/31588]
2025-08-13 17:00:49,576 - INFO - Epoch 14, Batch 340: loss: 12.183256  [21760/31588]
2025-08-13 17:00:49,816 - INFO - Epoch 14, Batch 360: loss: 13.170520  [23040/31588]
2025-08-13 17:00:50,118 - INFO - Epoch 14, Batch 380: loss: 12.046955  [24320/31588]
2025-08-13 17:00:50,353 - INFO - Epoch 14, Batch 400: loss: 12.668267  [25600/31588]
2025-08-13 17:00:50,575 - INFO - Epoch 14, Batch 420: loss: 12.551303  [26880/31588]
2025-08-13 17:00:50,878 - INFO - Epoch 14, Batch 440: loss: 12.589435  [28160/31588]
2025-08-13 17:00:51,163 - INFO - Epoch 14, Batch 460: loss: 12.961172  [29440/31588]
2025-08-13 17:00:51,416 - INFO - Epoch 14, Batch 480: loss: 13.060911  [30720/31588]
2025-08-13 17:00:51,651 - INFO - Epoch 14 Training Summary:
2025-08-13 17:00:51,652 - INFO -   Average Loss: 12.426035
2025-08-13 17:00:51,652 - INFO -   Accuracy: 0.9717
2025-08-13 17:00:51,652 - INFO -   F1 (macro): 0.9717
2025-08-13 17:00:51,652 - INFO -   Epoch Time: 6.59s
2025-08-13 17:00:51,652 - INFO -   Average Batch Time: 0.0075s
2025-08-13 17:00:51,652 - INFO -   Samples/second: 4796.0
2025-08-13 17:00:53,453 - INFO - Epoch 14 Complete:
2025-08-13 17:00:53,453 - INFO -   Train Loss: 12.426035, Val Loss: 12.653681
2025-08-13 17:00:53,453 - INFO -   Train Acc: 0.9717, Val Acc: 0.9579
2025-08-13 17:00:53,453 - INFO -   Epoch Time: 8.44s
2025-08-13 17:00:53,455 - INFO - EarlyStopping counter: 1 out of 5 (Epoch 14)
2025-08-13 17:00:53,676 - INFO - Epoch 15, Batch 0: loss: 13.080564  [    0/31588]
2025-08-13 17:00:53,910 - INFO - Epoch 15, Batch 20: loss: 11.841428  [ 1280/31588]
2025-08-13 17:00:54,121 - INFO - Epoch 15, Batch 40: loss: 12.238845  [ 2560/31588]
2025-08-13 17:00:54,441 - INFO - Epoch 15, Batch 60: loss: 12.545741  [ 3840/31588]
2025-08-13 17:00:54,816 - INFO - Epoch 15, Batch 80: loss: 11.875283  [ 5120/31588]
2025-08-13 17:00:55,083 - INFO - Epoch 15, Batch 100: loss: 12.946181  [ 6400/31588]
2025-08-13 17:00:55,373 - INFO - Epoch 15, Batch 120: loss: 12.326121  [ 7680/31588]
2025-08-13 17:00:55,703 - INFO - Epoch 15, Batch 140: loss: 12.245311  [ 8960/31588]
2025-08-13 17:00:55,939 - INFO - Epoch 15, Batch 160: loss: 12.613940  [10240/31588]
2025-08-13 17:00:56,291 - INFO - Epoch 15, Batch 180: loss: 12.915620  [11520/31588]
2025-08-13 17:00:56,526 - INFO - Epoch 15, Batch 200: loss: 12.548876  [12800/31588]
2025-08-13 17:00:56,840 - INFO - Epoch 15, Batch 220: loss: 12.264268  [14080/31588]
2025-08-13 17:00:57,074 - INFO - Epoch 15, Batch 240: loss: 12.121694  [15360/31588]
2025-08-13 17:00:57,292 - INFO - Epoch 15, Batch 260: loss: 12.083442  [16640/31588]
2025-08-13 17:00:57,581 - INFO - Epoch 15, Batch 280: loss: 11.914124  [17920/31588]
2025-08-13 17:00:57,828 - INFO - Epoch 15, Batch 300: loss: 11.846378  [19200/31588]
2025-08-13 17:00:58,184 - INFO - Epoch 15, Batch 320: loss: 12.867932  [20480/31588]
2025-08-13 17:00:58,403 - INFO - Epoch 15, Batch 340: loss: 12.175180  [21760/31588]
2025-08-13 17:00:58,699 - INFO - Epoch 15, Batch 360: loss: 11.998448  [23040/31588]
2025-08-13 17:00:58,900 - INFO - Epoch 15, Batch 380: loss: 12.144835  [24320/31588]
2025-08-13 17:00:59,114 - INFO - Epoch 15, Batch 400: loss: 12.214595  [25600/31588]
2025-08-13 17:00:59,386 - INFO - Epoch 15, Batch 420: loss: 12.676279  [26880/31588]
2025-08-13 17:00:59,592 - INFO - Epoch 15, Batch 440: loss: 12.174858  [28160/31588]
2025-08-13 17:00:59,845 - INFO - Epoch 15, Batch 460: loss: 12.486158  [29440/31588]
2025-08-13 17:01:00,030 - INFO - Epoch 15, Batch 480: loss: 12.668122  [30720/31588]
2025-08-13 17:01:00,212 - INFO - Epoch 15 Training Summary:
2025-08-13 17:01:00,212 - INFO -   Average Loss: 12.384769
2025-08-13 17:01:00,212 - INFO -   Accuracy: 0.9735
2025-08-13 17:01:00,212 - INFO -   F1 (macro): 0.9735
2025-08-13 17:01:00,212 - INFO -   Epoch Time: 6.72s
2025-08-13 17:01:00,212 - INFO -   Average Batch Time: 0.0067s
2025-08-13 17:01:00,212 - INFO -   Samples/second: 4699.2
2025-08-13 17:01:01,722 - INFO - Epoch 15 Complete:
2025-08-13 17:01:01,722 - INFO -   Train Loss: 12.384769, Val Loss: 12.606228
2025-08-13 17:01:01,723 - INFO -   Train Acc: 0.9735, Val Acc: 0.9583
2025-08-13 17:01:01,723 - INFO -   Epoch Time: 8.27s
2025-08-13 17:01:01,724 - INFO - EarlyStopping counter: 2 out of 5 (Epoch 15)
2025-08-13 17:01:01,960 - INFO - Epoch 16, Batch 0: loss: 12.506921  [    0/31588]
2025-08-13 17:01:02,197 - INFO - Epoch 16, Batch 20: loss: 11.870562  [ 1280/31588]
2025-08-13 17:01:02,427 - INFO - Epoch 16, Batch 40: loss: 11.942080  [ 2560/31588]
2025-08-13 17:01:02,750 - INFO - Epoch 16, Batch 60: loss: 12.160542  [ 3840/31588]
2025-08-13 17:01:02,987 - INFO - Epoch 16, Batch 80: loss: 12.429311  [ 5120/31588]
2025-08-13 17:01:03,311 - INFO - Epoch 16, Batch 100: loss: 12.502420  [ 6400/31588]
2025-08-13 17:01:03,526 - INFO - Epoch 16, Batch 120: loss: 11.846363  [ 7680/31588]
2025-08-13 17:01:03,778 - INFO - Epoch 16, Batch 140: loss: 12.563726  [ 8960/31588]
2025-08-13 17:01:04,104 - INFO - Epoch 16, Batch 160: loss: 12.598668  [10240/31588]
2025-08-13 17:01:04,354 - INFO - Epoch 16, Batch 180: loss: 12.152879  [11520/31588]
2025-08-13 17:01:04,604 - INFO - Epoch 16, Batch 200: loss: 12.270072  [12800/31588]
2025-08-13 17:01:04,920 - INFO - Epoch 16, Batch 220: loss: 12.843688  [14080/31588]
2025-08-13 17:01:05,145 - INFO - Epoch 16, Batch 240: loss: 11.862077  [15360/31588]
2025-08-13 17:01:05,476 - INFO - Epoch 16, Batch 260: loss: 12.664128  [16640/31588]
2025-08-13 17:01:05,721 - INFO - Epoch 16, Batch 280: loss: 12.761311  [17920/31588]
2025-08-13 17:01:06,038 - INFO - Epoch 16, Batch 300: loss: 12.432671  [19200/31588]
2025-08-13 17:01:06,282 - INFO - Epoch 16, Batch 320: loss: 12.591761  [20480/31588]
2025-08-13 17:01:06,544 - INFO - Epoch 16, Batch 340: loss: 12.795347  [21760/31588]
2025-08-13 17:01:06,848 - INFO - Epoch 16, Batch 360: loss: 12.263108  [23040/31588]
2025-08-13 17:01:07,088 - INFO - Epoch 16, Batch 380: loss: 12.171720  [24320/31588]
2025-08-13 17:01:07,394 - INFO - Epoch 16, Batch 400: loss: 12.371854  [25600/31588]
2025-08-13 17:01:07,609 - INFO - Epoch 16, Batch 420: loss: 11.891557  [26880/31588]
2025-08-13 17:01:07,885 - INFO - Epoch 16, Batch 440: loss: 14.036929  [28160/31588]
2025-08-13 17:01:08,074 - INFO - Epoch 16, Batch 460: loss: 11.906254  [29440/31588]
2025-08-13 17:01:08,245 - INFO - Epoch 16, Batch 480: loss: 13.189918  [30720/31588]
2025-08-13 17:01:08,448 - INFO - Epoch 16 Training Summary:
2025-08-13 17:01:08,448 - INFO -   Average Loss: 12.368547
2025-08-13 17:01:08,448 - INFO -   Accuracy: 0.9743
2025-08-13 17:01:08,448 - INFO -   F1 (macro): 0.9742
2025-08-13 17:01:08,448 - INFO -   Epoch Time: 6.69s
2025-08-13 17:01:08,448 - INFO -   Average Batch Time: 0.0066s
2025-08-13 17:01:08,448 - INFO -   Samples/second: 4718.5
2025-08-13 17:01:10,051 - INFO - Epoch 16 Complete:
2025-08-13 17:01:10,051 - INFO -   Train Loss: 12.368547, Val Loss: 12.621578
2025-08-13 17:01:10,051 - INFO -   Train Acc: 0.9743, Val Acc: 0.9586
2025-08-13 17:01:10,051 - INFO -   Epoch Time: 8.33s
2025-08-13 17:01:10,053 - INFO - EarlyStopping counter: 3 out of 5 (Epoch 16)
2025-08-13 17:01:10,262 - INFO - Epoch 17, Batch 0: loss: 12.162772  [    0/31588]
2025-08-13 17:01:10,513 - INFO - Epoch 17, Batch 20: loss: 12.716676  [ 1280/31588]
2025-08-13 17:01:10,736 - INFO - Epoch 17, Batch 40: loss: 12.149269  [ 2560/31588]
2025-08-13 17:01:11,084 - INFO - Epoch 17, Batch 60: loss: 12.164980  [ 3840/31588]
2025-08-13 17:01:11,303 - INFO - Epoch 17, Batch 80: loss: 12.303646  [ 5120/31588]
2025-08-13 17:01:11,580 - INFO - Epoch 17, Batch 100: loss: 12.169592  [ 6400/31588]
2025-08-13 17:01:11,801 - INFO - Epoch 17, Batch 120: loss: 12.407256  [ 7680/31588]
2025-08-13 17:01:12,088 - INFO - Epoch 17, Batch 140: loss: 12.632946  [ 8960/31588]
2025-08-13 17:01:12,355 - INFO - Epoch 17, Batch 160: loss: 12.448292  [10240/31588]
2025-08-13 17:01:12,552 - INFO - Epoch 17, Batch 180: loss: 12.179439  [11520/31588]
2025-08-13 17:01:12,843 - INFO - Epoch 17, Batch 200: loss: 12.386755  [12800/31588]
2025-08-13 17:01:13,043 - INFO - Epoch 17, Batch 220: loss: 12.184012  [14080/31588]
2025-08-13 17:01:13,341 - INFO - Epoch 17, Batch 240: loss: 11.872945  [15360/31588]
2025-08-13 17:01:13,570 - INFO - Epoch 17, Batch 260: loss: 11.871340  [16640/31588]
2025-08-13 17:01:13,842 - INFO - Epoch 17, Batch 280: loss: 12.577728  [17920/31588]
2025-08-13 17:01:14,049 - INFO - Epoch 17, Batch 300: loss: 12.227267  [19200/31588]
2025-08-13 17:01:14,272 - INFO - Epoch 17, Batch 320: loss: 12.418513  [20480/31588]
2025-08-13 17:01:14,561 - INFO - Epoch 17, Batch 340: loss: 13.199627  [21760/31588]
2025-08-13 17:01:14,768 - INFO - Epoch 17, Batch 360: loss: 13.084107  [23040/31588]
2025-08-13 17:01:15,010 - INFO - Epoch 17, Batch 380: loss: 12.840431  [24320/31588]
2025-08-13 17:01:15,247 - INFO - Epoch 17, Batch 400: loss: 12.201009  [25600/31588]
2025-08-13 17:01:15,479 - INFO - Epoch 17, Batch 420: loss: 12.857968  [26880/31588]
2025-08-13 17:01:15,788 - INFO - Epoch 17, Batch 440: loss: 12.186586  [28160/31588]
2025-08-13 17:01:16,011 - INFO - Epoch 17, Batch 460: loss: 12.337368  [29440/31588]
2025-08-13 17:01:16,282 - INFO - Epoch 17, Batch 480: loss: 12.480941  [30720/31588]
2025-08-13 17:01:16,424 - INFO - Epoch 17 Training Summary:
2025-08-13 17:01:16,425 - INFO -   Average Loss: 12.355876
2025-08-13 17:01:16,425 - INFO -   Accuracy: 0.9746
2025-08-13 17:01:16,425 - INFO -   F1 (macro): 0.9746
2025-08-13 17:01:16,425 - INFO -   Epoch Time: 6.33s
2025-08-13 17:01:16,425 - INFO -   Average Batch Time: 0.0066s
2025-08-13 17:01:16,425 - INFO -   Samples/second: 4986.4
2025-08-13 17:01:18,115 - INFO - Learning rate reduced from 1.00e-03 to 5.00e-04
2025-08-13 17:01:18,115 - INFO - Epoch 17 Complete:
2025-08-13 17:01:18,115 - INFO -   Train Loss: 12.355876, Val Loss: 12.645229
2025-08-13 17:01:18,115 - INFO -   Train Acc: 0.9746, Val Acc: 0.9567
2025-08-13 17:01:18,116 - INFO -   Epoch Time: 8.06s
2025-08-13 17:01:18,117 - INFO - EarlyStopping counter: 4 out of 5 (Epoch 17)
2025-08-13 17:01:18,339 - INFO - Epoch 18, Batch 0: loss: 13.123797  [    0/31588]
2025-08-13 17:01:18,610 - INFO - Epoch 18, Batch 20: loss: 12.833259  [ 1280/31588]
2025-08-13 17:01:18,882 - INFO - Epoch 18, Batch 40: loss: 11.834244  [ 2560/31588]
2025-08-13 17:01:19,144 - INFO - Epoch 18, Batch 60: loss: 11.994370  [ 3840/31588]
2025-08-13 17:01:19,506 - INFO - Epoch 18, Batch 80: loss: 12.168281  [ 5120/31588]
2025-08-13 17:01:19,797 - INFO - Epoch 18, Batch 100: loss: 12.217819  [ 6400/31588]
2025-08-13 17:01:20,100 - INFO - Epoch 18, Batch 120: loss: 12.538684  [ 7680/31588]
2025-08-13 17:01:20,307 - INFO - Epoch 18, Batch 140: loss: 12.523527  [ 8960/31588]
2025-08-13 17:01:20,601 - INFO - Epoch 18, Batch 160: loss: 12.550172  [10240/31588]
2025-08-13 17:01:20,817 - INFO - Epoch 18, Batch 180: loss: 12.497801  [11520/31588]
2025-08-13 17:01:21,032 - INFO - Epoch 18, Batch 200: loss: 12.597672  [12800/31588]
2025-08-13 17:01:21,352 - INFO - Epoch 18, Batch 220: loss: 12.469378  [14080/31588]
2025-08-13 17:01:21,556 - INFO - Epoch 18, Batch 240: loss: 12.174835  [15360/31588]
2025-08-13 17:01:21,855 - INFO - Epoch 18, Batch 260: loss: 11.845530  [16640/31588]
2025-08-13 17:01:22,080 - INFO - Epoch 18, Batch 280: loss: 11.982275  [17920/31588]
2025-08-13 17:01:22,353 - INFO - Epoch 18, Batch 300: loss: 12.356055  [19200/31588]
2025-08-13 17:01:22,559 - INFO - Epoch 18, Batch 320: loss: 12.865545  [20480/31588]
2025-08-13 17:01:22,768 - INFO - Epoch 18, Batch 340: loss: 11.837421  [21760/31588]
2025-08-13 17:01:23,061 - INFO - Epoch 18, Batch 360: loss: 11.866502  [23040/31588]
2025-08-13 17:01:23,260 - INFO - Epoch 18, Batch 380: loss: 12.279380  [24320/31588]
2025-08-13 17:01:23,531 - INFO - Epoch 18, Batch 400: loss: 12.167686  [25600/31588]
2025-08-13 17:01:23,741 - INFO - Epoch 18, Batch 420: loss: 12.849821  [26880/31588]
2025-08-13 17:01:24,025 - INFO - Epoch 18, Batch 440: loss: 12.831994  [28160/31588]
2025-08-13 17:01:24,204 - INFO - Epoch 18, Batch 460: loss: 12.475448  [29440/31588]
2025-08-13 17:01:24,497 - INFO - Epoch 18, Batch 480: loss: 12.178025  [30720/31588]
2025-08-13 17:01:24,690 - INFO - Epoch 18 Training Summary:
2025-08-13 17:01:24,691 - INFO -   Average Loss: 12.376055
2025-08-13 17:01:24,691 - INFO -   Accuracy: 0.9739
2025-08-13 17:01:24,691 - INFO -   F1 (macro): 0.9739
2025-08-13 17:01:24,691 - INFO -   Epoch Time: 6.54s
2025-08-13 17:01:24,691 - INFO -   Average Batch Time: 0.0068s
2025-08-13 17:01:24,691 - INFO -   Samples/second: 4832.8
2025-08-13 17:01:26,291 - INFO - Epoch 18 Complete:
2025-08-13 17:01:26,291 - INFO -   Train Loss: 12.376055, Val Loss: 12.612432
2025-08-13 17:01:26,291 - INFO -   Train Acc: 0.9739, Val Acc: 0.9590
2025-08-13 17:01:26,291 - INFO -   Epoch Time: 8.17s
2025-08-13 17:01:26,293 - INFO - EarlyStopping counter: 5 out of 5 (Epoch 18)
2025-08-13 17:01:26,293 - INFO - Early stopping triggered. Saving final model to final_model.pth (Epoch 18)
2025-08-13 17:01:26,322 - INFO - Early stopping triggered at epoch 18
2025-08-13 17:01:26,322 - INFO - Training completed in 147.08 seconds
